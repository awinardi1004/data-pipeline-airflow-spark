[2025-10-17T10:54:43.687+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: test_hdfs_spark.hdfs_spark_test manual__2025-10-17T10:54:10.557988+00:00 [queued]>
[2025-10-17T10:54:43.777+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: test_hdfs_spark.hdfs_spark_test manual__2025-10-17T10:54:10.557988+00:00 [queued]>
[2025-10-17T10:54:43.778+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2025-10-17T10:54:43.872+0000] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): hdfs_spark_test> on 2025-10-17 10:54:10.557988+00:00
[2025-10-17T10:54:43.897+0000] {standard_task_runner.py:60} INFO - Started process 1770 to run task
[2025-10-17T10:54:43.915+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'test_hdfs_spark', 'hdfs_spark_test', 'manual__2025-10-17T10:54:10.557988+00:00', '--job-id', '72', '--raw', '--subdir', 'DAGS_FOLDER/test_hdfs_spark.py', '--cfg-path', '/tmp/tmp98vb2wfw']
[2025-10-17T10:54:43.929+0000] {standard_task_runner.py:88} INFO - Job 72: Subtask hdfs_spark_test
[2025-10-17T10:54:44.221+0000] {task_command.py:423} INFO - Running <TaskInstance: test_hdfs_spark.hdfs_spark_test manual__2025-10-17T10:54:10.557988+00:00 [running]> on host 3e60b84d081a
[2025-10-17T10:54:44.615+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='test_hdfs_spark' AIRFLOW_CTX_TASK_ID='hdfs_spark_test' AIRFLOW_CTX_EXECUTION_DATE='2025-10-17T10:54:10.557988+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-17T10:54:10.557988+00:00'
[2025-10-17T10:54:44.656+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2025-10-17T10:54:44.662+0000] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark --verbose --queue default /opt/***/dags/hdfs_spark_test.py
[2025-10-17T10:55:04.118+0000] {spark_submit.py:492} INFO - Using properties file: null
[2025-10-17T10:55:05.420+0000] {spark_submit.py:492} INFO - Parsed arguments:
[2025-10-17T10:55:05.421+0000] {spark_submit.py:492} INFO - master                  local
[2025-10-17T10:55:05.422+0000] {spark_submit.py:492} INFO - remote                  null
[2025-10-17T10:55:05.428+0000] {spark_submit.py:492} INFO - deployMode              null
[2025-10-17T10:55:05.444+0000] {spark_submit.py:492} INFO - executorMemory          null
[2025-10-17T10:55:05.512+0000] {spark_submit.py:492} INFO - executorCores           null
[2025-10-17T10:55:05.519+0000] {spark_submit.py:492} INFO - totalExecutorCores      null
[2025-10-17T10:55:05.520+0000] {spark_submit.py:492} INFO - propertiesFile          null
[2025-10-17T10:55:05.522+0000] {spark_submit.py:492} INFO - driverMemory            null
[2025-10-17T10:55:05.528+0000] {spark_submit.py:492} INFO - driverCores             null
[2025-10-17T10:55:05.529+0000] {spark_submit.py:492} INFO - driverExtraClassPath    null
[2025-10-17T10:55:05.530+0000] {spark_submit.py:492} INFO - driverExtraLibraryPath  null
[2025-10-17T10:55:05.535+0000] {spark_submit.py:492} INFO - driverExtraJavaOptions  null
[2025-10-17T10:55:05.536+0000] {spark_submit.py:492} INFO - supervise               false
[2025-10-17T10:55:05.537+0000] {spark_submit.py:492} INFO - queue                   default
[2025-10-17T10:55:05.541+0000] {spark_submit.py:492} INFO - numExecutors            null
[2025-10-17T10:55:05.543+0000] {spark_submit.py:492} INFO - files                   null
[2025-10-17T10:55:05.616+0000] {spark_submit.py:492} INFO - pyFiles                 null
[2025-10-17T10:55:05.620+0000] {spark_submit.py:492} INFO - archives                null
[2025-10-17T10:55:05.621+0000] {spark_submit.py:492} INFO - mainClass               null
[2025-10-17T10:55:05.623+0000] {spark_submit.py:492} INFO - primaryResource         file:/opt/***/dags/hdfs_spark_test.py
[2025-10-17T10:55:05.626+0000] {spark_submit.py:492} INFO - name                    arrow-spark
[2025-10-17T10:55:05.639+0000] {spark_submit.py:492} INFO - childArgs               []
[2025-10-17T10:55:05.640+0000] {spark_submit.py:492} INFO - jars                    null
[2025-10-17T10:55:05.645+0000] {spark_submit.py:492} INFO - packages                null
[2025-10-17T10:55:05.649+0000] {spark_submit.py:492} INFO - packagesExclusions      null
[2025-10-17T10:55:05.650+0000] {spark_submit.py:492} INFO - repositories            null
[2025-10-17T10:55:05.711+0000] {spark_submit.py:492} INFO - verbose                 true
[2025-10-17T10:55:05.712+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:55:05.713+0000] {spark_submit.py:492} INFO - Spark properties used, including those specified through
[2025-10-17T10:55:05.724+0000] {spark_submit.py:492} INFO - --conf and those from the properties file null:
[2025-10-17T10:55:05.728+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:55:05.729+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:55:05.731+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:55:08.511+0000] {spark_submit.py:492} INFO - Main class:
[2025-10-17T10:55:08.516+0000] {spark_submit.py:492} INFO - org.apache.spark.deploy.PythonRunner
[2025-10-17T10:55:08.518+0000] {spark_submit.py:492} INFO - Arguments:
[2025-10-17T10:55:08.535+0000] {spark_submit.py:492} INFO - file:/opt/***/dags/hdfs_spark_test.py
[2025-10-17T10:55:08.546+0000] {spark_submit.py:492} INFO - null
[2025-10-17T10:55:08.549+0000] {spark_submit.py:492} INFO - Spark config:
[2025-10-17T10:55:08.611+0000] {spark_submit.py:492} INFO - (spark.app.name,arrow-spark)
[2025-10-17T10:55:08.618+0000] {spark_submit.py:492} INFO - (spark.app.submitTime,1760698508227)
[2025-10-17T10:55:08.619+0000] {spark_submit.py:492} INFO - (spark.master,local)
[2025-10-17T10:55:08.627+0000] {spark_submit.py:492} INFO - (spark.submit.deployMode,client)
[2025-10-17T10:55:08.631+0000] {spark_submit.py:492} INFO - (spark.submit.pyFiles,)
[2025-10-17T10:55:08.715+0000] {spark_submit.py:492} INFO - Classpath elements:
[2025-10-17T10:55:08.718+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:55:08.720+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:55:08.721+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:55:14.613+0000] {spark_submit.py:492} INFO - Starting Spark HDFS Test...
[2025-10-17T10:55:15.215+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:15 INFO SparkContext: Running Spark version 3.5.1
[2025-10-17T10:55:15.221+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:15 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-10-17T10:55:15.222+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:15 INFO SparkContext: Java version 17.0.16
[2025-10-17T10:55:15.738+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:15 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-10-17T10:55:16.819+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:16 INFO ResourceUtils: ==============================================================
[2025-10-17T10:55:16.824+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:16 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-10-17T10:55:16.825+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:16 INFO ResourceUtils: ==============================================================
[2025-10-17T10:55:16.827+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:16 INFO SparkContext: Submitted application: HDFS-Test
[2025-10-17T10:55:16.936+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:16 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-10-17T10:55:17.013+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:17 INFO ResourceProfile: Limiting resource is cpu
[2025-10-17T10:55:17.014+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:17 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-10-17T10:55:17.418+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:17 INFO SecurityManager: Changing view acls to: ***
[2025-10-17T10:55:17.425+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:17 INFO SecurityManager: Changing modify acls to: ***
[2025-10-17T10:55:17.427+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:17 INFO SecurityManager: Changing view acls groups to:
[2025-10-17T10:55:17.430+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:17 INFO SecurityManager: Changing modify acls groups to:
[2025-10-17T10:55:17.436+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:17 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-10-17T10:55:21.744+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:21 INFO Utils: Successfully started service 'sparkDriver' on port 33857.
[2025-10-17T10:55:21.965+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:21 INFO SparkEnv: Registering MapOutputTracker
[2025-10-17T10:55:22.346+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:22 INFO SparkEnv: Registering BlockManagerMaster
[2025-10-17T10:55:22.576+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-10-17T10:55:22.582+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-10-17T10:55:22.646+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:22 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-10-17T10:55:22.800+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0ceee551-b195-4d0e-85ab-879686d85b98
[2025-10-17T10:55:22.882+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:22 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
[2025-10-17T10:55:23.080+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:23 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-10-17T10:55:24.074+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:24 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-10-17T10:55:24.370+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:24 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-10-17T10:55:25.065+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO Executor: Starting executor ID driver on host 3e60b84d081a
[2025-10-17T10:55:25.074+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-10-17T10:55:25.075+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO Executor: Java version 17.0.16
[2025-10-17T10:55:25.093+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-10-17T10:55:25.094+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1c590b88 for default.
[2025-10-17T10:55:25.282+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33259.
[2025-10-17T10:55:25.283+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO NettyBlockTransferService: Server created on 3e60b84d081a:33259
[2025-10-17T10:55:25.288+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-10-17T10:55:25.379+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 3e60b84d081a, 33259, None)
[2025-10-17T10:55:25.448+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO BlockManagerMasterEndpoint: Registering block manager 3e60b84d081a:33259 with 413.9 MiB RAM, BlockManagerId(driver, 3e60b84d081a, 33259, None)
[2025-10-17T10:55:25.451+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 3e60b84d081a, 33259, None)
[2025-10-17T10:55:25.453+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 3e60b84d081a, 33259, None)
[2025-10-17T10:55:27.953+0000] {spark_submit.py:492} INFO - Trying to read from: hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv
[2025-10-17T10:55:28.346+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:28 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-10-17T10:55:28.372+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:28 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-10-17T10:55:40.256+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:40 INFO InMemoryFileIndex: It took 767 ms to list leaf files for 1 paths.
[2025-10-17T10:55:41.350+0000] {spark_submit.py:492} INFO - 25/10/17 10:55:41 INFO InMemoryFileIndex: It took 100 ms to list leaf files for 1 paths.
[2025-10-17T10:56:07.555+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:07 INFO FileSourceStrategy: Pushed Filters:
[2025-10-17T10:56:07.566+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:07 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2025-10-17T10:56:10.966+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:10 INFO CodeGenerator: Code generated in 1453.361195 ms
[2025-10-17T10:56:11.224+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:11 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.0 KiB, free 413.7 MiB)
[2025-10-17T10:56:11.404+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:11 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 413.7 MiB)
[2025-10-17T10:56:11.469+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:11 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 3e60b84d081a:33259 (size: 34.5 KiB, free: 413.9 MiB)
[2025-10-17T10:56:11.482+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:11 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2025-10-17T10:56:11.514+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13228261 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-17T10:56:12.507+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:12 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-10-17T10:56:12.780+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:12 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-17T10:56:12.795+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:12 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2025-10-17T10:56:12.805+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:12 INFO DAGScheduler: Parents of final stage: List()
[2025-10-17T10:56:12.806+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:12 INFO DAGScheduler: Missing parents: List()
[2025-10-17T10:56:12.846+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:12 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-17T10:56:13.672+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:13 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 413.7 MiB)
[2025-10-17T10:56:13.705+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:13 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 413.7 MiB)
[2025-10-17T10:56:13.708+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:13 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 3e60b84d081a:33259 (size: 6.4 KiB, free: 413.9 MiB)
[2025-10-17T10:56:13.710+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:13 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-10-17T10:56:13.792+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-17T10:56:13.794+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-10-17T10:56:14.094+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:14 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (3e60b84d081a, executor driver, partition 0, ANY, 8262 bytes)
[2025-10-17T10:56:14.173+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:14 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-10-17T10:56:15.075+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:15 INFO CodeGenerator: Code generated in 189.082771 ms
[2025-10-17T10:56:15.082+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:15 INFO FileScanRDD: Reading File path: hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv, range: 0-9033957, partition values: [empty row]
[2025-10-17T10:56:15.186+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:15 INFO CodeGenerator: Code generated in 89.194837 ms
[2025-10-17T10:56:15.470+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:15 WARN BlockReaderFactory: I/O error constructing remote block reader.
[2025-10-17T10:56:15.474+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T10:56:15.479+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T10:56:15.480+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T10:56:15.483+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T10:56:15.484+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T10:56:15.485+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T10:56:15.486+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T10:56:15.488+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T10:56:15.489+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T10:56:15.489+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T10:56:15.490+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T10:56:15.491+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T10:56:15.492+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:15.492+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:15.493+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:15.494+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:15.495+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:15.496+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:15.497+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:15.498+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:15.499+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:15.500+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:15.500+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:15.501+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:15.502+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:15.503+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:15.504+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:15.505+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:15.506+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:15.506+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:15.507+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:15.508+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:15.566+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:15.566+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:15.567+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:15.568+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:15.569+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:15.569+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:15.570+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:15.570+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:15.571+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:15.572+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:15.573+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:15.574+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:15.575+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:15.576+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:15.577+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:15.577+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:15.578+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:15.579+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:15 WARN DFSClient: Failed to connect to /127.0.0.1:9866 for file /user/winardi/data/olist/olist_customers_dataset.csv for block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020, add to deadNodes and continue.
[2025-10-17T10:56:15.580+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T10:56:15.581+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T10:56:15.582+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T10:56:15.583+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T10:56:15.583+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T10:56:15.584+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T10:56:15.585+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T10:56:15.586+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T10:56:15.587+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T10:56:15.588+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T10:56:15.589+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T10:56:15.590+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T10:56:15.591+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:15.592+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:15.593+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:15.594+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:15.595+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:15.596+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:15.596+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:15.597+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:15.598+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:15.598+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:15.599+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:15.600+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:15.600+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:15.601+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:15.602+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:15.604+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:15.605+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:15.605+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:15.606+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:15.607+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:15.608+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:15.609+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:15.609+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:15.610+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:15.611+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:15.612+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:15.612+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:15.613+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:15.614+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:15.615+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:15.616+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:15.616+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:15.617+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:15.619+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:15.620+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:15.621+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:15.623+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:15.666+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:15 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T10:56:15.667+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:15 INFO DFSClient: Could not obtain BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Will get new block locations from namenode and retry...
[2025-10-17T10:56:15.667+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:15 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 2492.5286757955632 msec.
[2025-10-17T10:56:17.988+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:17 WARN BlockReaderFactory: I/O error constructing remote block reader.
[2025-10-17T10:56:18.005+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T10:56:18.007+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T10:56:18.008+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T10:56:18.010+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T10:56:18.011+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T10:56:18.069+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T10:56:18.071+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T10:56:18.079+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T10:56:18.080+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T10:56:18.087+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T10:56:18.089+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T10:56:18.090+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T10:56:18.095+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:18.100+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:18.103+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:18.104+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:18.116+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:18.120+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:18.125+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:18.127+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:18.132+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:18.135+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:18.135+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:18.137+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:18.138+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:18.138+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:18.140+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:18.141+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:18.142+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:18.166+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:18.167+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:18.168+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:18.169+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:18.172+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:18.174+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:18.179+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:18.182+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:18.188+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:18.195+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:18.195+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:18.196+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:18.197+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:18.198+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:18.202+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:18.207+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:18.208+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:18.208+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:18.209+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:18.210+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:18.211+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:17 WARN DFSClient: Failed to connect to /127.0.0.1:9866 for file /user/winardi/data/olist/olist_customers_dataset.csv for block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020, add to deadNodes and continue.
[2025-10-17T10:56:18.211+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T10:56:18.212+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T10:56:18.213+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T10:56:18.214+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T10:56:18.215+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T10:56:18.217+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T10:56:18.218+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T10:56:18.219+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T10:56:18.219+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T10:56:18.220+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T10:56:18.221+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T10:56:18.221+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T10:56:18.222+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:18.222+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:18.223+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:18.223+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:18.224+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:18.224+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:18.225+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:18.225+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:18.226+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:18.227+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:18.227+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:18.227+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:18.231+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:18.266+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:18.267+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:18.267+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:18.268+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:18.269+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:18.270+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:18.270+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:18.272+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:18.272+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:18.273+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:18.274+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:18.275+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:18.275+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:18.276+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:18.277+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:18.278+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:18.279+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:18.280+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:18.280+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:18.281+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:18.282+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:18.282+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:18.283+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:18.284+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:18.285+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:17 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T10:56:18.286+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:17 INFO DFSClient: Could not obtain BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Will get new block locations from namenode and retry...
[2025-10-17T10:56:18.287+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:18 WARN DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 3072.5080680374826 msec.
[2025-10-17T10:56:21.083+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:21 WARN BlockReaderFactory: I/O error constructing remote block reader.
[2025-10-17T10:56:21.101+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T10:56:21.108+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T10:56:21.111+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T10:56:21.166+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T10:56:21.204+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T10:56:21.205+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T10:56:21.266+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T10:56:21.277+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T10:56:21.280+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T10:56:21.282+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T10:56:21.283+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T10:56:21.292+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T10:56:21.295+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:21.297+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:21.315+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:21.337+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:21.345+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:21.346+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:21.348+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:21.350+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:21.351+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:21.352+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:21.353+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:21.354+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:21.356+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:21.357+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:21.358+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:21.380+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:21.387+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:21.389+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:21.398+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:21.400+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:21.401+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:21.403+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:21.404+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:21.407+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:21.415+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:21.418+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:21.421+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:21.427+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:21.428+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:21.429+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:21.430+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:21.431+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:21.432+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:21.434+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:21.435+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:21.436+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:21.504+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:21.520+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:21 WARN DFSClient: Failed to connect to /127.0.0.1:9866 for file /user/winardi/data/olist/olist_customers_dataset.csv for block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020, add to deadNodes and continue.
[2025-10-17T10:56:21.522+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T10:56:21.524+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T10:56:21.525+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T10:56:21.526+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T10:56:21.527+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T10:56:21.528+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T10:56:21.566+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T10:56:21.567+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T10:56:21.568+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T10:56:21.570+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T10:56:21.571+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T10:56:21.573+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T10:56:21.574+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:21.575+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:21.577+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:21.579+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:21.580+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:21.581+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:21.588+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:21.600+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:21.615+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:21.620+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:21.621+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:21.629+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:21.635+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:21.636+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:21.639+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:21.640+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:21.642+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:21.643+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:21.666+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:21.692+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:21.701+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:21.716+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:21.733+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:21.736+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:21.740+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:21.766+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:21.772+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:21.772+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:21.774+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:21.778+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:21.780+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:21.782+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:21.783+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:21.787+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:21.793+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:21.796+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:21.800+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:21.801+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:21 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T10:56:21.803+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:21 INFO DFSClient: Could not obtain BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Will get new block locations from namenode and retry...
[2025-10-17T10:56:21.813+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:21 WARN DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 13441.994350116627 msec.
[2025-10-17T10:56:37.266+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:37 WARN BlockReaderFactory: I/O error constructing remote block reader.
[2025-10-17T10:56:37.272+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T10:56:37.273+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T10:56:37.276+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T10:56:37.277+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T10:56:37.278+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T10:56:37.279+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T10:56:37.280+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T10:56:37.282+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T10:56:37.283+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T10:56:37.284+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T10:56:37.297+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T10:56:37.300+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T10:56:37.301+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:37.302+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:37.307+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:37.309+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:37.311+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:37.312+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:37.313+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:37.314+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:37.315+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:37.316+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:37.318+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:37.319+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:37.320+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:37.321+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:37.322+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:37.328+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:37.329+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:37.330+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:37.332+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:37.333+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:37.334+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:37.335+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:37.337+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:37.338+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:37.340+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:37.342+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:37.344+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:37.345+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:37.395+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:37.398+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:37.404+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:37.406+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:37.407+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:37.408+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:37.409+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:37.411+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:37.413+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:37.414+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:37 WARN DFSClient: Failed to connect to /127.0.0.1:9866 for file /user/winardi/data/olist/olist_customers_dataset.csv for block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020, add to deadNodes and continue.
[2025-10-17T10:56:37.420+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T10:56:37.423+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T10:56:37.429+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T10:56:37.430+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T10:56:37.431+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T10:56:37.432+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T10:56:37.495+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T10:56:37.501+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T10:56:37.503+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T10:56:37.505+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T10:56:37.506+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T10:56:37.508+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T10:56:37.509+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:37.510+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:37.510+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:37.512+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:37.522+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:37.524+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:37.525+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:37.526+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:37.527+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:37.528+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:37.529+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:37.531+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:37.532+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:37.595+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:37.602+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:37.604+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:37.605+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:37.606+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:37.607+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:37.608+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:37.609+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:37.610+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:37.618+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:37.620+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:37.622+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:37.625+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:37.638+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:37.647+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:37.650+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:37.695+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:37.696+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:37.715+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:37.721+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:37.722+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:37.724+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:37.738+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:37.739+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:37.795+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:37 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T10:56:37.816+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:37 WARN DFSClient: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Throwing a BlockMissingException
[2025-10-17T10:56:37.817+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:37 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T10:56:37.820+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:37 WARN DFSClient: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Throwing a BlockMissingException
[2025-10-17T10:56:37.895+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:37 WARN DFSClient: DFS Read
[2025-10-17T10:56:37.909+0000] {spark_submit.py:492} INFO - org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:37.919+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:37.923+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:37.995+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:38.013+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:38.020+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:38.024+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:38.095+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:38.102+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:38.108+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:38.110+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:38.111+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:38.114+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:38.120+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:38.124+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:38.195+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:38.198+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:38.199+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:38.200+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:38.202+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:38.203+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:38.204+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:38.205+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:38.215+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:38.219+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:38.224+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:38.225+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:38.295+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:38.296+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:38.297+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:38.300+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:38.301+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:38.302+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:38.303+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:38.304+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:38.305+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:38.306+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:38.307+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:38.308+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:38.309+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:38.311+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:38.312+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:38.324+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:38 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
[2025-10-17T10:56:38.325+0000] {spark_submit.py:492} INFO - org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv. Details:
[2025-10-17T10:56:38.326+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T10:56:38.327+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T10:56:38.394+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:38.396+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:38.397+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:38.398+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:38.399+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:38.400+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:38.401+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:38.403+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:38.404+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:38.407+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:38.408+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:38.409+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:38.411+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:38.412+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:38.413+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:38.414+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:38.415+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:38.416+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:38.417+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:38.417+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:38.418+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:38.419+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:38.420+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:38.422+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:38.423+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:38.424+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:38.424+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:38.425+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:38.426+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:38.427+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:38.427+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:38.428+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:38.429+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:38.429+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:38.430+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:38.431+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:38.432+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:38.433+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:38.494+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:38.495+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:38.496+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:38.507+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:38.508+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T10:56:38.626+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:38 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (3e60b84d081a executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv. Details:
[2025-10-17T10:56:38.703+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T10:56:38.709+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T10:56:38.717+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:38.728+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:38.736+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:38.744+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:38.747+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:38.751+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:38.752+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:38.795+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:38.797+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:38.798+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:38.801+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:38.808+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:38.810+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:38.814+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:38.816+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:38.817+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:38.819+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:38.821+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:38.828+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:38.831+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:38.837+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:38.841+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:38.898+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:38.911+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:38.914+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:38.919+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:38.928+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:38.929+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:38.934+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:38.935+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:38.943+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:38.951+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:38.958+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:38.958+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:38.994+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:38.997+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:38.999+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:39.007+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:39.009+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:39.023+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:39.039+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:39.041+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:39.044+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T10:56:39.095+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:56:39.096+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:38 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[2025-10-17T10:56:39.097+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-10-17T10:56:39.102+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:38 INFO TaskSchedulerImpl: Cancelling stage 0
[2025-10-17T10:56:39.104+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (3e60b84d081a executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv. Details:
[2025-10-17T10:56:39.112+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T10:56:39.120+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T10:56:39.121+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:39.128+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:39.142+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:39.144+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:39.145+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:39.152+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:39.232+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:39.245+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:39.253+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:39.295+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:39.296+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:39.297+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:39.298+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:39.300+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:39.302+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:39.304+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:39.310+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:39.311+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:39.312+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:39.314+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:39.315+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:39.316+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:39.317+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:39.318+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:39.320+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:39.323+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:39.324+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:39.325+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:39.334+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:39.342+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:39.344+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:39.404+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:39.406+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:39.412+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:39.417+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:39.427+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:39.428+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:39.442+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:39.496+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:39.501+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:39.502+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:39.503+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:39.504+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T10:56:39.507+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:56:39.509+0000] {spark_submit.py:492} INFO - Driver stacktrace:
[2025-10-17T10:56:39.509+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:38 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) failed in 25.816 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (3e60b84d081a executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv. Details:
[2025-10-17T10:56:39.510+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T10:56:39.511+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T10:56:39.513+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:39.513+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:39.514+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:39.515+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:39.516+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:39.517+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:39.518+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:39.519+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:39.521+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:39.522+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:39.523+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:39.524+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:39.525+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:39.526+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:39.527+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:39.528+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:39.529+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:39.534+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:39.540+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:39.595+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:39.611+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:39.630+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:39.637+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:39.638+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:39.695+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:39.699+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:39.701+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:39.702+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:39.703+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:39.704+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:39.706+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:39.707+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:39.712+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:39.718+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:39.720+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:39.721+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:39.722+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:39.723+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:39.724+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:39.795+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:39.802+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:39.803+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:39.803+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T10:56:39.806+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:56:39.808+0000] {spark_submit.py:492} INFO - Driver stacktrace:
[2025-10-17T10:56:39.815+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:38 INFO DAGScheduler: Job 0 failed: csv at NativeMethodAccessorImpl.java:0, took 23.684700 s
[2025-10-17T10:56:40.304+0000] {spark_submit.py:492} INFO - ❌ ERROR: An error occurred while calling o29.csv.
[2025-10-17T10:56:40.325+0000] {spark_submit.py:492} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (3e60b84d081a executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv. Details:
[2025-10-17T10:56:40.351+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T10:56:40.354+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T10:56:40.357+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:40.358+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:40.359+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:40.395+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:40.404+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:40.407+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:40.408+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:40.409+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:40.411+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:40.413+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:40.414+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:40.415+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:40.417+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:40.424+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:40.430+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:40.431+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:40.433+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:40.434+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:40.436+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:40.500+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:40.503+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:40.505+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:40.506+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:40.507+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:40.508+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:40.509+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:40.510+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:40.511+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:40.512+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:40.515+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:40.516+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:40.520+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:40.596+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:40.597+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:40.598+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:40.599+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:40.600+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:40.601+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:40.602+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:40.603+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:40.604+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:40.606+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:40.607+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T10:56:40.609+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:56:40.612+0000] {spark_submit.py:492} INFO - Driver stacktrace:
[2025-10-17T10:56:40.612+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-10-17T10:56:40.613+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-10-17T10:56:40.614+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-10-17T10:56:40.615+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-10-17T10:56:40.616+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-10-17T10:56:40.618+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-10-17T10:56:40.619+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-10-17T10:56:40.622+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-10-17T10:56:40.623+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-10-17T10:56:40.696+0000] {spark_submit.py:492} INFO - at scala.Option.foreach(Option.scala:407)
[2025-10-17T10:56:40.698+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-10-17T10:56:40.699+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-10-17T10:56:40.700+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-10-17T10:56:40.701+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-10-17T10:56:40.702+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-10-17T10:56:40.704+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-10-17T10:56:40.705+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2025-10-17T10:56:40.706+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
[2025-10-17T10:56:40.707+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
[2025-10-17T10:56:40.708+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
[2025-10-17T10:56:40.709+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
[2025-10-17T10:56:40.710+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
[2025-10-17T10:56:40.713+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)
[2025-10-17T10:56:40.715+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)
[2025-10-17T10:56:40.717+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)
[2025-10-17T10:56:40.719+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-10-17T10:56:40.720+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)
[2025-10-17T10:56:40.721+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-10-17T10:56:40.722+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-10-17T10:56:40.723+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-10-17T10:56:40.724+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-10-17T10:56:40.725+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-10-17T10:56:40.726+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)
[2025-10-17T10:56:40.728+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.head(Dataset.scala:3314)
[2025-10-17T10:56:40.795+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.take(Dataset.scala:3537)
[2025-10-17T10:56:40.802+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
[2025-10-17T10:56:40.803+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
[2025-10-17T10:56:40.804+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
[2025-10-17T10:56:40.805+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
[2025-10-17T10:56:40.806+0000] {spark_submit.py:492} INFO - at scala.Option.orElse(Option.scala:447)
[2025-10-17T10:56:40.807+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
[2025-10-17T10:56:40.808+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
[2025-10-17T10:56:40.809+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
[2025-10-17T10:56:40.810+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
[2025-10-17T10:56:40.816+0000] {spark_submit.py:492} INFO - at scala.Option.getOrElse(Option.scala:189)
[2025-10-17T10:56:40.819+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
[2025-10-17T10:56:40.820+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
[2025-10-17T10:56:40.821+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-10-17T10:56:40.822+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-10-17T10:56:40.823+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-10-17T10:56:40.825+0000] {spark_submit.py:492} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-10-17T10:56:40.826+0000] {spark_submit.py:492} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-10-17T10:56:40.827+0000] {spark_submit.py:492} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-10-17T10:56:40.828+0000] {spark_submit.py:492} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-10-17T10:56:40.828+0000] {spark_submit.py:492} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-10-17T10:56:40.829+0000] {spark_submit.py:492} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-10-17T10:56:40.895+0000] {spark_submit.py:492} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-10-17T10:56:40.899+0000] {spark_submit.py:492} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-10-17T10:56:40.900+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:40.902+0000] {spark_submit.py:492} INFO - Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv. Details:
[2025-10-17T10:56:40.903+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T10:56:40.904+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T10:56:40.904+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:40.906+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:40.907+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:40.908+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:40.909+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:40.909+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:40.914+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:40.915+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:40.916+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:40.916+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:40.917+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:40.918+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:40.918+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:40.919+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:40.920+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:40.921+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:40.922+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:40.923+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:40.924+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:40.925+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:40.927+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:40.929+0000] {spark_submit.py:492} INFO - ... 1 more
[2025-10-17T10:56:40.929+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:40.930+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:40.931+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:40.932+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:40.933+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:40.934+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:40.935+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:40.936+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:40.937+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:40.995+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:41.008+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:41.012+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:41.025+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:41.027+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:41.028+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:41.031+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:41.035+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:41.036+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:41.037+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:41.038+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:41.039+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T10:56:41.040+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:56:41.042+0000] {spark_submit.py:492} INFO - Traceback (most recent call last):
[2025-10-17T10:56:41.043+0000] {spark_submit.py:492} INFO - File "/opt/***/dags/hdfs_spark_test.py", line 20, in main
[2025-10-17T10:56:41.050+0000] {spark_submit.py:492} INFO - df = spark.read.csv(hdfs_path, header=True, inferSchema=True)
[2025-10-17T10:56:41.052+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 740, in csv
[2025-10-17T10:56:41.054+0000] {spark_submit.py:492} INFO - return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
[2025-10-17T10:56:41.055+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-10-17T10:56:41.056+0000] {spark_submit.py:492} INFO - return_value = get_return_value(
[2025-10-17T10:56:41.057+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
[2025-10-17T10:56:41.068+0000] {spark_submit.py:492} INFO - return f(*a, **kw)
[2025-10-17T10:56:41.095+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-10-17T10:56:41.098+0000] {spark_submit.py:492} INFO - raise Py4JJavaError(
[2025-10-17T10:56:41.106+0000] {spark_submit.py:492} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
[2025-10-17T10:56:41.107+0000] {spark_submit.py:492} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (3e60b84d081a executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv. Details:
[2025-10-17T10:56:41.108+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T10:56:41.111+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T10:56:41.112+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:41.113+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:41.114+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:41.115+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:41.117+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:41.118+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:41.119+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:41.121+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:41.122+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:41.124+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:41.126+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:41.127+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:41.128+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:41.129+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:41.130+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:41.131+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:41.132+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:41.133+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:41.134+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:41.136+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:41.137+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:41.138+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:41.142+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:41.143+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:41.144+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:41.145+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:41.146+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:41.148+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:41.198+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:41.203+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:41.204+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:41.208+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:41.211+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:41.217+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:41.224+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:41.227+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:41.294+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:41.310+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:41.325+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:41.348+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:41.351+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:41.360+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:41.365+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T10:56:41.376+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:56:41.386+0000] {spark_submit.py:492} INFO - Driver stacktrace:
[2025-10-17T10:56:41.395+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-10-17T10:56:41.402+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-10-17T10:56:41.410+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-10-17T10:56:41.418+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-10-17T10:56:41.419+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-10-17T10:56:41.422+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-10-17T10:56:41.425+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-10-17T10:56:41.427+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-10-17T10:56:41.449+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-10-17T10:56:41.458+0000] {spark_submit.py:492} INFO - at scala.Option.foreach(Option.scala:407)
[2025-10-17T10:56:41.460+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-10-17T10:56:41.463+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-10-17T10:56:41.466+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-10-17T10:56:41.473+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-10-17T10:56:41.478+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-10-17T10:56:41.489+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-10-17T10:56:41.495+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2025-10-17T10:56:41.496+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
[2025-10-17T10:56:41.503+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
[2025-10-17T10:56:41.507+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
[2025-10-17T10:56:41.510+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
[2025-10-17T10:56:41.512+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
[2025-10-17T10:56:41.514+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)
[2025-10-17T10:56:41.516+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)
[2025-10-17T10:56:41.523+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)
[2025-10-17T10:56:41.525+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-10-17T10:56:41.526+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)
[2025-10-17T10:56:41.528+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-10-17T10:56:41.530+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-10-17T10:56:41.532+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-10-17T10:56:41.533+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-10-17T10:56:41.534+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-10-17T10:56:41.536+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)
[2025-10-17T10:56:41.539+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.head(Dataset.scala:3314)
[2025-10-17T10:56:41.543+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.take(Dataset.scala:3537)
[2025-10-17T10:56:41.544+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
[2025-10-17T10:56:41.545+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
[2025-10-17T10:56:41.547+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
[2025-10-17T10:56:41.548+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
[2025-10-17T10:56:41.549+0000] {spark_submit.py:492} INFO - at scala.Option.orElse(Option.scala:447)
[2025-10-17T10:56:41.550+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
[2025-10-17T10:56:41.551+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
[2025-10-17T10:56:41.554+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
[2025-10-17T10:56:41.555+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
[2025-10-17T10:56:41.556+0000] {spark_submit.py:492} INFO - at scala.Option.getOrElse(Option.scala:189)
[2025-10-17T10:56:41.557+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
[2025-10-17T10:56:41.558+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
[2025-10-17T10:56:41.560+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-10-17T10:56:41.561+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-10-17T10:56:41.563+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-10-17T10:56:41.603+0000] {spark_submit.py:492} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-10-17T10:56:41.604+0000] {spark_submit.py:492} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-10-17T10:56:41.606+0000] {spark_submit.py:492} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-10-17T10:56:41.607+0000] {spark_submit.py:492} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-10-17T10:56:41.608+0000] {spark_submit.py:492} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-10-17T10:56:41.610+0000] {spark_submit.py:492} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-10-17T10:56:41.611+0000] {spark_submit.py:492} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-10-17T10:56:41.612+0000] {spark_submit.py:492} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-10-17T10:56:41.614+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T10:56:41.615+0000] {spark_submit.py:492} INFO - Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/data/olist/olist_customers_dataset.csv. Details:
[2025-10-17T10:56:41.616+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T10:56:41.617+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T10:56:41.618+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:41.618+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:41.619+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T10:56:41.620+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T10:56:41.621+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T10:56:41.622+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T10:56:41.622+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T10:56:41.623+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T10:56:41.624+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T10:56:41.625+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T10:56:41.626+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T10:56:41.627+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T10:56:41.629+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T10:56:41.630+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T10:56:41.631+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T10:56:41.632+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T10:56:41.633+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T10:56:41.634+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T10:56:41.636+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T10:56:41.637+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T10:56:41.638+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T10:56:41.639+0000] {spark_submit.py:492} INFO - ... 1 more
[2025-10-17T10:56:41.641+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741844_1020 file=/user/winardi/data/olist/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T10:56:41.642+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T10:56:41.643+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T10:56:41.646+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T10:56:41.647+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T10:56:41.704+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T10:56:41.707+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T10:56:41.708+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T10:56:41.710+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T10:56:41.722+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T10:56:41.724+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T10:56:41.732+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T10:56:41.736+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T10:56:41.740+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T10:56:41.749+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T10:56:41.753+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T10:56:41.755+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:41.758+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T10:56:41.762+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T10:56:41.764+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T10:56:41.805+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T10:56:41.808+0000] {spark_submit.py:492} INFO - 
[2025-10-17T10:56:41.810+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:40 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-10-17T10:56:41.814+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:40 INFO SparkUI: Stopped Spark web UI at http://3e60b84d081a:4040
[2025-10-17T10:56:41.814+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:40 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-10-17T10:56:41.816+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:40 INFO MemoryStore: MemoryStore cleared
[2025-10-17T10:56:41.820+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:40 INFO BlockManager: BlockManager stopped
[2025-10-17T10:56:41.821+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:40 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-10-17T10:56:41.822+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:40 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-10-17T10:56:41.823+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:40 INFO SparkContext: Successfully stopped SparkContext
[2025-10-17T10:56:41.824+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:41 INFO ShutdownHookManager: Shutdown hook called
[2025-10-17T10:56:41.825+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a2b3e0df-4421-444f-b1ff-61dd35bc883a
[2025-10-17T10:56:41.828+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a0df9d28-d710-47d4-8a93-3ab253bd1e7a
[2025-10-17T10:56:41.835+0000] {spark_submit.py:492} INFO - 25/10/17 10:56:41 INFO ShutdownHookManager: Deleting directory /tmp/spark-a0df9d28-d710-47d4-8a93-3ab253bd1e7a/pyspark-ad0cd05c-dff0-41c6-9972-050482489b38
[2025-10-17T10:56:42.145+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 423, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark --verbose --queue default /opt/***/dags/hdfs_spark_test.py. Error code is: 1.
[2025-10-17T10:56:42.194+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=test_hdfs_spark, task_id=hdfs_spark_test, execution_date=20251017T105410, start_date=20251017T105443, end_date=20251017T105642
[2025-10-17T10:56:42.273+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 72 for task hdfs_spark_test (Cannot execute: spark-submit --master local --name arrow-spark --verbose --queue default /opt/***/dags/hdfs_spark_test.py. Error code is: 1.; 1770)
[2025-10-17T10:56:42.340+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-10-17T10:56:42.433+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
