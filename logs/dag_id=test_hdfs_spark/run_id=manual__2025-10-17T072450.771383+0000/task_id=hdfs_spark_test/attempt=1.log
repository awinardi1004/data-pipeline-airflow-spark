[2025-10-17T07:25:21.756+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: test_hdfs_spark.hdfs_spark_test manual__2025-10-17T07:24:50.771383+00:00 [queued]>
[2025-10-17T07:25:22.248+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: test_hdfs_spark.hdfs_spark_test manual__2025-10-17T07:24:50.771383+00:00 [queued]>
[2025-10-17T07:25:22.258+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2025-10-17T07:25:22.386+0000] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): hdfs_spark_test> on 2025-10-17 07:24:50.771383+00:00
[2025-10-17T07:25:22.449+0000] {standard_task_runner.py:60} INFO - Started process 1177 to run task
[2025-10-17T07:25:22.477+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'test_hdfs_spark', 'hdfs_spark_test', 'manual__2025-10-17T07:24:50.771383+00:00', '--job-id', '54', '--raw', '--subdir', 'DAGS_FOLDER/test_hdfs_spark.py', '--cfg-path', '/tmp/tmp2y8qqgd9']
[2025-10-17T07:25:22.554+0000] {standard_task_runner.py:88} INFO - Job 54: Subtask hdfs_spark_test
[2025-10-17T07:25:23.477+0000] {task_command.py:423} INFO - Running <TaskInstance: test_hdfs_spark.hdfs_spark_test manual__2025-10-17T07:24:50.771383+00:00 [running]> on host 673fe1495c96
[2025-10-17T07:25:25.889+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='test_hdfs_spark' AIRFLOW_CTX_TASK_ID='hdfs_spark_test' AIRFLOW_CTX_EXECUTION_DATE='2025-10-17T07:24:50.771383+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-17T07:24:50.771383+00:00'
[2025-10-17T07:25:26.218+0000] {base.py:83} INFO - Using connection ID 'spark_default' for task execution.
[2025-10-17T07:25:26.259+0000] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark --verbose --queue default /opt/***/dags/hdfs_spark_test.py
[2025-10-17T07:25:57.273+0000] {spark_submit.py:492} INFO - Using properties file: null
[2025-10-17T07:25:58.576+0000] {spark_submit.py:492} INFO - Parsed arguments:
[2025-10-17T07:25:58.577+0000] {spark_submit.py:492} INFO - master                  local
[2025-10-17T07:25:58.583+0000] {spark_submit.py:492} INFO - remote                  null
[2025-10-17T07:25:58.587+0000] {spark_submit.py:492} INFO - deployMode              null
[2025-10-17T07:25:58.588+0000] {spark_submit.py:492} INFO - executorMemory          null
[2025-10-17T07:25:58.589+0000] {spark_submit.py:492} INFO - executorCores           null
[2025-10-17T07:25:58.590+0000] {spark_submit.py:492} INFO - totalExecutorCores      null
[2025-10-17T07:25:58.662+0000] {spark_submit.py:492} INFO - propertiesFile          null
[2025-10-17T07:25:58.663+0000] {spark_submit.py:492} INFO - driverMemory            null
[2025-10-17T07:25:58.667+0000] {spark_submit.py:492} INFO - driverCores             null
[2025-10-17T07:25:58.668+0000] {spark_submit.py:492} INFO - driverExtraClassPath    null
[2025-10-17T07:25:58.669+0000] {spark_submit.py:492} INFO - driverExtraLibraryPath  null
[2025-10-17T07:25:58.670+0000] {spark_submit.py:492} INFO - driverExtraJavaOptions  null
[2025-10-17T07:25:58.672+0000] {spark_submit.py:492} INFO - supervise               false
[2025-10-17T07:25:58.674+0000] {spark_submit.py:492} INFO - queue                   default
[2025-10-17T07:25:58.675+0000] {spark_submit.py:492} INFO - numExecutors            null
[2025-10-17T07:25:58.677+0000] {spark_submit.py:492} INFO - files                   null
[2025-10-17T07:25:58.680+0000] {spark_submit.py:492} INFO - pyFiles                 null
[2025-10-17T07:25:58.681+0000] {spark_submit.py:492} INFO - archives                null
[2025-10-17T07:25:58.685+0000] {spark_submit.py:492} INFO - mainClass               null
[2025-10-17T07:25:58.686+0000] {spark_submit.py:492} INFO - primaryResource         file:/opt/***/dags/hdfs_spark_test.py
[2025-10-17T07:25:58.768+0000] {spark_submit.py:492} INFO - name                    arrow-spark
[2025-10-17T07:25:58.769+0000] {spark_submit.py:492} INFO - childArgs               []
[2025-10-17T07:25:58.771+0000] {spark_submit.py:492} INFO - jars                    null
[2025-10-17T07:25:58.772+0000] {spark_submit.py:492} INFO - packages                null
[2025-10-17T07:25:58.783+0000] {spark_submit.py:492} INFO - packagesExclusions      null
[2025-10-17T07:25:58.784+0000] {spark_submit.py:492} INFO - repositories            null
[2025-10-17T07:25:58.789+0000] {spark_submit.py:492} INFO - verbose                 true
[2025-10-17T07:25:58.793+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:25:58.861+0000] {spark_submit.py:492} INFO - Spark properties used, including those specified through
[2025-10-17T07:25:58.872+0000] {spark_submit.py:492} INFO - --conf and those from the properties file null:
[2025-10-17T07:25:58.884+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:25:58.897+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:25:58.899+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:26:01.661+0000] {spark_submit.py:492} INFO - Main class:
[2025-10-17T07:26:01.672+0000] {spark_submit.py:492} INFO - org.apache.spark.deploy.PythonRunner
[2025-10-17T07:26:01.675+0000] {spark_submit.py:492} INFO - Arguments:
[2025-10-17T07:26:01.679+0000] {spark_submit.py:492} INFO - file:/opt/***/dags/hdfs_spark_test.py
[2025-10-17T07:26:01.680+0000] {spark_submit.py:492} INFO - null
[2025-10-17T07:26:01.761+0000] {spark_submit.py:492} INFO - Spark config:
[2025-10-17T07:26:01.772+0000] {spark_submit.py:492} INFO - (spark.app.name,arrow-spark)
[2025-10-17T07:26:01.786+0000] {spark_submit.py:492} INFO - (spark.app.submitTime,1760685961282)
[2025-10-17T07:26:01.788+0000] {spark_submit.py:492} INFO - (spark.master,local)
[2025-10-17T07:26:01.862+0000] {spark_submit.py:492} INFO - (spark.submit.deployMode,client)
[2025-10-17T07:26:01.873+0000] {spark_submit.py:492} INFO - (spark.submit.pyFiles,)
[2025-10-17T07:26:01.874+0000] {spark_submit.py:492} INFO - Classpath elements:
[2025-10-17T07:26:01.875+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:26:01.876+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:26:01.877+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:26:09.039+0000] {spark_submit.py:492} INFO - Starting Spark HDFS Test...
[2025-10-17T07:26:09.848+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:09 INFO SparkContext: Running Spark version 3.5.1
[2025-10-17T07:26:09.857+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:09 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-10-17T07:26:09.906+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:09 INFO SparkContext: Java version 17.0.16
[2025-10-17T07:26:10.523+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-10-17T07:26:11.634+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:11 INFO ResourceUtils: ==============================================================
[2025-10-17T07:26:11.705+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:11 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-10-17T07:26:11.718+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:11 INFO ResourceUtils: ==============================================================
[2025-10-17T07:26:11.803+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:11 INFO SparkContext: Submitted application: HDFS-Test
[2025-10-17T07:26:11.905+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:11 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-10-17T07:26:11.941+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:11 INFO ResourceProfile: Limiting resource is cpu
[2025-10-17T07:26:12.005+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:11 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-10-17T07:26:12.608+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:12 INFO SecurityManager: Changing view acls to: ***
[2025-10-17T07:26:12.703+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:12 INFO SecurityManager: Changing modify acls to: ***
[2025-10-17T07:26:12.717+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:12 INFO SecurityManager: Changing view acls groups to:
[2025-10-17T07:26:12.805+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:12 INFO SecurityManager: Changing modify acls groups to:
[2025-10-17T07:26:12.809+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:12 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-10-17T07:26:15.533+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:15 INFO Utils: Successfully started service 'sparkDriver' on port 44383.
[2025-10-17T07:26:16.118+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:16 INFO SparkEnv: Registering MapOutputTracker
[2025-10-17T07:26:16.517+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:16 INFO SparkEnv: Registering BlockManagerMaster
[2025-10-17T07:26:16.708+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:16 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-10-17T07:26:16.728+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:16 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-10-17T07:26:16.742+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:16 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-10-17T07:26:17.053+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:17 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1f5f2ae5-8cbd-4bab-b0b2-0eac9a06ca5c
[2025-10-17T07:26:17.312+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:17 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
[2025-10-17T07:26:17.605+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:17 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-10-17T07:26:18.449+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:18 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-10-17T07:26:19.041+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-10-17T07:26:20.623+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:20 INFO Executor: Starting executor ID driver on host 673fe1495c96
[2025-10-17T07:26:20.645+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:20 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-10-17T07:26:20.709+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:20 INFO Executor: Java version 17.0.16
[2025-10-17T07:26:20.729+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:20 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-10-17T07:26:20.735+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:20 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@3685045b for default.
[2025-10-17T07:26:21.042+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:21 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35293.
[2025-10-17T07:26:21.049+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:21 INFO NettyBlockTransferService: Server created on 673fe1495c96:35293
[2025-10-17T07:26:21.103+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:21 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-10-17T07:26:21.123+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:21 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 673fe1495c96, 35293, None)
[2025-10-17T07:26:21.138+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:21 INFO BlockManagerMasterEndpoint: Registering block manager 673fe1495c96:35293 with 413.9 MiB RAM, BlockManagerId(driver, 673fe1495c96, 35293, None)
[2025-10-17T07:26:21.145+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:21 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 673fe1495c96, 35293, None)
[2025-10-17T07:26:21.146+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:21 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 673fe1495c96, 35293, None)
[2025-10-17T07:26:24.751+0000] {spark_submit.py:492} INFO - Trying to read from: hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv
[2025-10-17T07:26:25.903+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:25 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-10-17T07:26:25.946+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:25 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-10-17T07:26:44.047+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:44 INFO InMemoryFileIndex: It took 1217 ms to list leaf files for 1 paths.
[2025-10-17T07:26:45.123+0000] {spark_submit.py:492} INFO - 25/10/17 07:26:45 INFO InMemoryFileIndex: It took 24 ms to list leaf files for 1 paths.
[2025-10-17T07:27:05.736+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:05 INFO FileSourceStrategy: Pushed Filters:
[2025-10-17T07:27:05.741+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:05 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)
[2025-10-17T07:27:13.972+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:13 INFO CodeGenerator: Code generated in 1995.176101 ms
[2025-10-17T07:27:14.253+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:14 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 200.0 KiB, free 413.7 MiB)
[2025-10-17T07:27:14.760+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:14 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 413.7 MiB)
[2025-10-17T07:27:14.780+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:14 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 673fe1495c96:35293 (size: 34.5 KiB, free: 413.9 MiB)
[2025-10-17T07:27:14.798+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:14 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0
[2025-10-17T07:27:14.912+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:14 INFO FileSourceScanExec: Planning scan with bin packing, max size: 13228261 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-17T07:27:15.468+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:15 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-10-17T07:27:15.587+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:15 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-17T07:27:15.590+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:15 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)
[2025-10-17T07:27:15.591+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:15 INFO DAGScheduler: Parents of final stage: List()
[2025-10-17T07:27:15.592+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:15 INFO DAGScheduler: Missing parents: List()
[2025-10-17T07:27:15.662+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:15 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-17T07:27:16.873+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:16 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 413.7 MiB)
[2025-10-17T07:27:17.065+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 413.7 MiB)
[2025-10-17T07:27:17.102+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 673fe1495c96:35293 (size: 6.4 KiB, free: 413.9 MiB)
[2025-10-17T07:27:17.112+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-10-17T07:27:17.467+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-17T07:27:17.511+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:17 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-10-17T07:27:18.386+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:18 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (673fe1495c96, executor driver, partition 0, ANY, 8257 bytes)
[2025-10-17T07:27:18.763+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:18 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-10-17T07:27:20.883+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:20 INFO CodeGenerator: Code generated in 123.869372 ms
[2025-10-17T07:27:20.957+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:20 INFO FileScanRDD: Reading File path: hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv, range: 0-9033957, partition values: [empty row]
[2025-10-17T07:27:21.163+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:21 INFO CodeGenerator: Code generated in 106.055027 ms
[2025-10-17T07:27:21.983+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:21 WARN BlockReaderFactory: I/O error constructing remote block reader.
[2025-10-17T07:27:21.984+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T07:27:21.985+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T07:27:21.990+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T07:27:21.991+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T07:27:21.992+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T07:27:21.993+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T07:27:21.994+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T07:27:21.995+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T07:27:21.996+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T07:27:21.997+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T07:27:21.998+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T07:27:21.999+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T07:27:22.053+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:22.063+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:22.071+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:22.076+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:22.078+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:22.084+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:22.086+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:22.091+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:22.093+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:22.096+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:22.098+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:22.099+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:22.100+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:22.101+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:22.102+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:22.102+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:22.104+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:22.105+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:22.107+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:22.112+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:22.113+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:22.115+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:22.116+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:22.116+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:22.117+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:22.118+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:22.119+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:22.153+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:22.158+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:22.159+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:22.160+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:22.161+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:22.162+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:22.163+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:22.165+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:22.166+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:22.167+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:22.168+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:21 WARN DFSClient: Failed to connect to /127.0.0.1:9866 for file /user/winardi/input/olist_customers_dataset.csv for block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002, add to deadNodes and continue.
[2025-10-17T07:27:22.174+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T07:27:22.177+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T07:27:22.178+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T07:27:22.180+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T07:27:22.181+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T07:27:22.182+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T07:27:22.183+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T07:27:22.184+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T07:27:22.185+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T07:27:22.186+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T07:27:22.187+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T07:27:22.189+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T07:27:22.190+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:22.191+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:22.192+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:22.193+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:22.194+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:22.194+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:22.195+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:22.196+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:22.197+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:22.197+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:22.198+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:22.199+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:22.199+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:22.200+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:22.201+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:22.202+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:22.203+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:22.204+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:22.204+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:22.205+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:22.206+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:22.206+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:22.207+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:22.208+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:22.208+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:22.209+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:22.209+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:22.210+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:22.210+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:22.211+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:22.212+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:22.212+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:22.213+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:22.213+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:22.214+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:22.215+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:22.215+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:22.216+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:21 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T07:27:22.253+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:21 INFO DFSClient: Could not obtain BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Will get new block locations from namenode and retry...
[2025-10-17T07:27:22.256+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:21 WARN DFSClient: DFS chooseDataNode: got # 1 IOException, will wait for 1937.4859947080843 msec.
[2025-10-17T07:27:23.953+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:23 WARN BlockReaderFactory: I/O error constructing remote block reader.
[2025-10-17T07:27:23.955+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T07:27:23.956+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T07:27:23.957+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T07:27:23.959+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T07:27:23.960+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T07:27:23.961+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T07:27:23.961+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T07:27:23.962+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T07:27:23.963+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T07:27:23.964+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T07:27:23.965+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T07:27:23.966+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T07:27:23.967+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:23.968+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:23.969+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:23.970+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:23.970+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:23.971+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:23.972+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:23.975+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:23.976+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:23.977+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:23.978+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:23.979+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:23.980+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:23.980+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:23.981+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:23.982+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:23.983+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:23.984+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:23.985+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:23.986+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:23.987+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:23.988+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:23.989+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:23.990+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:23.991+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:23.992+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:23.993+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:23.993+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:23.995+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:23.996+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:23.997+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:23.999+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:24.000+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:24.001+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:24.002+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:24.003+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:24.054+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:24.055+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:23 WARN DFSClient: Failed to connect to /127.0.0.1:9866 for file /user/winardi/input/olist_customers_dataset.csv for block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002, add to deadNodes and continue.
[2025-10-17T07:27:24.056+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T07:27:24.056+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T07:27:24.057+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T07:27:24.058+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T07:27:24.059+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T07:27:24.060+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T07:27:24.061+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T07:27:24.062+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T07:27:24.063+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T07:27:24.064+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T07:27:24.065+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T07:27:24.066+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T07:27:24.067+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:24.068+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:24.069+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:24.070+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:24.071+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:24.071+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:24.072+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:24.073+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:24.073+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:24.074+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:24.075+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:24.076+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:24.076+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:24.077+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:24.078+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:24.079+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:24.082+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:24.083+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:24.084+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:24.085+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:24.086+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:24.087+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:24.088+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:24.089+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:24.090+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:24.090+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:24.091+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:24.091+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:24.092+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:24.093+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:24.093+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:24.094+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:24.094+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:24.096+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:24.097+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:24.098+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:24.099+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:24.101+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:23 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T07:27:24.101+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:23 INFO DFSClient: Could not obtain BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Will get new block locations from namenode and retry...
[2025-10-17T07:27:24.102+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:23 WARN DFSClient: DFS chooseDataNode: got # 2 IOException, will wait for 6047.215793591489 msec.
[2025-10-17T07:27:30.011+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:30 WARN BlockReaderFactory: I/O error constructing remote block reader.
[2025-10-17T07:27:30.014+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T07:27:30.015+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T07:27:30.016+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T07:27:30.017+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T07:27:30.018+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T07:27:30.019+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T07:27:30.020+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T07:27:30.020+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T07:27:30.021+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T07:27:30.023+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T07:27:30.025+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T07:27:30.026+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T07:27:30.028+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:30.029+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:30.030+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:30.031+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:30.032+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:30.035+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:30.036+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:30.041+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:30.042+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:30.043+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:30.044+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:30.045+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:30.046+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:30.048+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:30.051+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:30.053+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:30.055+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:30.057+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:30.058+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:30.059+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:30.060+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:30.061+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:30.062+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:30.062+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:30.065+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:30.066+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:30.066+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:30.067+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:30.068+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:30.069+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:30.070+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:30.071+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:30.072+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:30.072+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:30.073+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:30.074+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:30.074+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:30.076+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:30 WARN DFSClient: Failed to connect to /127.0.0.1:9866 for file /user/winardi/input/olist_customers_dataset.csv for block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002, add to deadNodes and continue.
[2025-10-17T07:27:30.076+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T07:27:30.077+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T07:27:30.078+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T07:27:30.078+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T07:27:30.079+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T07:27:30.079+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T07:27:30.080+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T07:27:30.080+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T07:27:30.081+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T07:27:30.082+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T07:27:30.082+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T07:27:30.083+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T07:27:30.084+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:30.084+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:30.085+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:30.087+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:30.088+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:30.088+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:30.089+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:30.090+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:30.090+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:30.091+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:30.091+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:30.092+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:30.093+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:30.094+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:30.095+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:30.096+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:30.096+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:30.097+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:30.097+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:30.098+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:30.099+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:30.100+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:30.100+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:30.101+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:30.101+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:30.102+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:30.102+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:30.103+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:30.104+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:30.104+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:30.105+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:30.105+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:30.106+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:30.106+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:30.107+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:30.108+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:30.108+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:30.109+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:30 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T07:27:30.109+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:30 INFO DFSClient: Could not obtain BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 from any node:  No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Will get new block locations from namenode and retry...
[2025-10-17T07:27:30.110+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:30 WARN DFSClient: DFS chooseDataNode: got # 3 IOException, will wait for 9019.107560900187 msec.
[2025-10-17T07:27:39.041+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:39 WARN BlockReaderFactory: I/O error constructing remote block reader.
[2025-10-17T07:27:39.049+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T07:27:39.051+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T07:27:39.052+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T07:27:39.063+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T07:27:39.066+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T07:27:39.067+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T07:27:39.068+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T07:27:39.081+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T07:27:39.084+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T07:27:39.085+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T07:27:39.087+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T07:27:39.088+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T07:27:39.089+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:39.090+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:39.101+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:39.103+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:39.105+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:39.107+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:39.111+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:39.113+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:39.114+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:39.115+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:39.117+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:39.118+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:39.119+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:39.120+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:39.163+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:39.169+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:39.172+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:39.173+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:39.177+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:39.179+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:39.180+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:39.181+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:39.182+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:39.184+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:39.185+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:39.191+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:39.195+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:39.197+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:39.197+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:39.199+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:39.253+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:39.255+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:39.256+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:39.257+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:39.258+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:39.261+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:39.262+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:39.263+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:39 WARN DFSClient: Failed to connect to /127.0.0.1:9866 for file /user/winardi/input/olist_customers_dataset.csv for block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002, add to deadNodes and continue.
[2025-10-17T07:27:39.264+0000] {spark_submit.py:492} INFO - java.net.ConnectException: Connection refused
[2025-10-17T07:27:39.265+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnect(Native Method)
[2025-10-17T07:27:39.266+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)
[2025-10-17T07:27:39.267+0000] {spark_submit.py:492} INFO - at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:946)
[2025-10-17T07:27:39.271+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:205)
[2025-10-17T07:27:39.272+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:586)
[2025-10-17T07:27:39.273+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSClient.newConnectedPeer(DFSClient.java:3033)
[2025-10-17T07:27:39.275+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.nextTcpPeer(BlockReaderFactory.java:829)
[2025-10-17T07:27:39.279+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.getRemoteBlockReaderFromTcp(BlockReaderFactory.java:754)
[2025-10-17T07:27:39.281+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.client.impl.BlockReaderFactory.build(BlockReaderFactory.java:381)
[2025-10-17T07:27:39.282+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.getBlockReader(DFSInputStream.java:755)
[2025-10-17T07:27:39.283+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:685)
[2025-10-17T07:27:39.284+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:39.285+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:39.286+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:39.287+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:39.288+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:39.288+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:39.289+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:39.290+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:39.291+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:39.294+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:39.295+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:39.296+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:39.298+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:39.299+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:39.353+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:39.358+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:39.362+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:39.364+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:39.370+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:39.372+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:39.378+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:39.380+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:39.381+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:39.382+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:39.383+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:39.384+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:39.385+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:39.386+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:39.387+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:39.389+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:39.390+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:39.393+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:39.396+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:39.397+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:39.399+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:39.400+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:39.401+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:39.402+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:39 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T07:27:39.403+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:39 WARN DFSClient: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Throwing a BlockMissingException
[2025-10-17T07:27:39.404+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:39 WARN DFSClient: No live nodes contain block BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 after checking nodes = [DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]], ignoredNodes = null
[2025-10-17T07:27:39.453+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:39 WARN DFSClient: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]. Throwing a BlockMissingException
[2025-10-17T07:27:39.461+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:39 WARN DFSClient: DFS Read
[2025-10-17T07:27:39.462+0000] {spark_submit.py:492} INFO - org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:39.464+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:39.465+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:39.467+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:39.477+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:39.478+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:39.482+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:39.484+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:39.485+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:39.487+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:39.723+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:39.732+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:39.738+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:39.739+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:39.740+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:39.837+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:39.843+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:39.847+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:39.848+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:39.921+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:42.399+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:42.403+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:42.404+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:42.404+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:42.405+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:42.480+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:42.485+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:42.486+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:42.490+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:42.491+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:42.492+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:42.493+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:42.494+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:42.496+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:42.500+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:42.502+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:42.503+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:42.504+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:42.508+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:42.509+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:42.510+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:42.515+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:42.651+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:42 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
[2025-10-17T07:27:42.653+0000] {spark_submit.py:492} INFO - org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv. Details:
[2025-10-17T07:27:42.655+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T07:27:42.656+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T07:27:42.658+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:42.663+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:42.694+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:42.708+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:42.710+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:42.718+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:42.722+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:42.725+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:42.730+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:42.732+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:42.733+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:42.736+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:42.740+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:42.741+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:42.744+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:42.745+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:42.746+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:42.747+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:42.780+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:42.781+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:42.782+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:42.782+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:42.783+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:42.784+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:42.785+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:42.787+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:42.790+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:42.796+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:42.797+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:42.799+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:42.801+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:42.802+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:42.806+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:42.807+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:42.809+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:42.810+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:42.811+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:42.812+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:42.813+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:42.814+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:42.820+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:42.821+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:42.824+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T07:27:42.997+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:42 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (673fe1495c96 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv. Details:
[2025-10-17T07:27:42.998+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T07:27:42.999+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T07:27:43.000+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:43.005+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.005+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:43.011+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:43.012+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:43.012+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:43.013+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:43.014+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:43.017+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:43.018+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:43.024+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:43.025+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:43.027+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:43.031+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:43.083+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:43.087+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:43.105+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:43.118+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:43.187+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:43.201+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:43.209+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:43.210+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:43.280+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:43.290+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:43.291+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:43.303+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:43.305+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:43.315+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:43.380+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:43.388+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:43.390+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:43.393+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:43.394+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:43.405+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:43.411+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:43.412+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:43.413+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:43.480+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:43.493+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.505+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.508+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:43.510+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:43.511+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T07:27:43.513+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:27:43.518+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:43 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
[2025-10-17T07:27:43.580+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:43 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-10-17T07:27:43.593+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:43 INFO TaskSchedulerImpl: Cancelling stage 0
[2025-10-17T07:27:43.609+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:43 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (673fe1495c96 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv. Details:
[2025-10-17T07:27:43.626+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T07:27:43.637+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T07:27:43.638+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:43.642+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.647+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:43.649+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:43.651+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:43.652+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:43.682+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:43.686+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:43.688+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:43.690+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:43.692+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:43.695+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:43.696+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:43.697+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:43.699+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:43.701+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:43.702+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:43.703+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:43.706+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:43.707+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:43.708+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:43.710+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:43.711+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:43.714+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:43.716+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:43.717+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:43.718+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:43.722+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:43.723+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:43.724+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:43.724+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:43.725+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:43.726+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:43.727+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:43.728+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:43.729+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:43.730+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:43.731+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:43.732+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.733+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.735+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:43.738+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:43.739+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T07:27:43.740+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:27:43.740+0000] {spark_submit.py:492} INFO - Driver stacktrace:
[2025-10-17T07:27:43.742+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:43 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) failed in 27.412 s due to Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (673fe1495c96 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv. Details:
[2025-10-17T07:27:43.743+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T07:27:43.744+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T07:27:43.745+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:43.746+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.750+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:43.752+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:43.753+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:43.755+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:43.756+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:43.757+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:43.758+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:43.759+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:43.760+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:43.761+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:43.761+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:43.762+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:43.763+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:43.763+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:43.764+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:43.764+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:43.782+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:43.784+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:43.786+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:43.788+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:43.789+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:43.790+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:43.791+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:43.794+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:43.795+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:43.799+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:43.802+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:43.803+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:43.805+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:43.806+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:43.810+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:43.813+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:43.814+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:43.815+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:43.817+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:43.818+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:43.819+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.820+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:43.820+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:43.821+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:43.822+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T07:27:43.823+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:27:43.824+0000] {spark_submit.py:492} INFO - Driver stacktrace:
[2025-10-17T07:27:43.825+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:43 INFO DAGScheduler: Job 0 failed: csv at NativeMethodAccessorImpl.java:0, took 25.111662 s
[2025-10-17T07:27:44.052+0000] {spark_submit.py:492} INFO - ❌ ERROR: An error occurred while calling o29.csv.
[2025-10-17T07:27:44.054+0000] {spark_submit.py:492} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (673fe1495c96 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv. Details:
[2025-10-17T07:27:44.057+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T07:27:44.060+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T07:27:44.063+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:44.065+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.067+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:44.069+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:44.073+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:44.074+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:44.076+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:44.077+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:44.079+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:44.081+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:44.084+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:44.085+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:44.091+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:44.092+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:44.093+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:44.098+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:44.104+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:44.105+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:44.107+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:44.107+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:44.108+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:44.109+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:44.110+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:44.111+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:44.112+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:44.113+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:44.114+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:44.115+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:44.115+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:44.116+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:44.117+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:44.119+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:44.121+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:44.122+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:44.124+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:44.126+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:44.127+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:44.128+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:44.129+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.130+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.131+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:44.133+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:44.134+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T07:27:44.135+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:27:44.181+0000] {spark_submit.py:492} INFO - Driver stacktrace:
[2025-10-17T07:27:44.183+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-10-17T07:27:44.185+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-10-17T07:27:44.186+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-10-17T07:27:44.187+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-10-17T07:27:44.189+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-10-17T07:27:44.191+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-10-17T07:27:44.192+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-10-17T07:27:44.194+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-10-17T07:27:44.196+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-10-17T07:27:44.197+0000] {spark_submit.py:492} INFO - at scala.Option.foreach(Option.scala:407)
[2025-10-17T07:27:44.198+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-10-17T07:27:44.200+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-10-17T07:27:44.201+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-10-17T07:27:44.202+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-10-17T07:27:44.203+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-10-17T07:27:44.204+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-10-17T07:27:44.208+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2025-10-17T07:27:44.212+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
[2025-10-17T07:27:44.213+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
[2025-10-17T07:27:44.217+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
[2025-10-17T07:27:44.219+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
[2025-10-17T07:27:44.220+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
[2025-10-17T07:27:44.221+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)
[2025-10-17T07:27:44.222+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)
[2025-10-17T07:27:44.223+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)
[2025-10-17T07:27:44.224+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-10-17T07:27:44.225+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)
[2025-10-17T07:27:44.226+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-10-17T07:27:44.227+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-10-17T07:27:44.228+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-10-17T07:27:44.282+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-10-17T07:27:44.287+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-10-17T07:27:44.289+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)
[2025-10-17T07:27:44.290+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.head(Dataset.scala:3314)
[2025-10-17T07:27:44.292+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.take(Dataset.scala:3537)
[2025-10-17T07:27:44.293+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
[2025-10-17T07:27:44.295+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
[2025-10-17T07:27:44.296+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
[2025-10-17T07:27:44.297+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
[2025-10-17T07:27:44.298+0000] {spark_submit.py:492} INFO - at scala.Option.orElse(Option.scala:447)
[2025-10-17T07:27:44.300+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
[2025-10-17T07:27:44.302+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
[2025-10-17T07:27:44.303+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
[2025-10-17T07:27:44.304+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
[2025-10-17T07:27:44.305+0000] {spark_submit.py:492} INFO - at scala.Option.getOrElse(Option.scala:189)
[2025-10-17T07:27:44.306+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
[2025-10-17T07:27:44.306+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
[2025-10-17T07:27:44.307+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-10-17T07:27:44.310+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-10-17T07:27:44.311+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-10-17T07:27:44.312+0000] {spark_submit.py:492} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-10-17T07:27:44.313+0000] {spark_submit.py:492} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-10-17T07:27:44.314+0000] {spark_submit.py:492} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-10-17T07:27:44.315+0000] {spark_submit.py:492} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-10-17T07:27:44.322+0000] {spark_submit.py:492} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-10-17T07:27:44.324+0000] {spark_submit.py:492} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-10-17T07:27:44.326+0000] {spark_submit.py:492} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-10-17T07:27:44.329+0000] {spark_submit.py:492} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-10-17T07:27:44.333+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:44.335+0000] {spark_submit.py:492} INFO - Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv. Details:
[2025-10-17T07:27:44.337+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T07:27:44.339+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T07:27:44.340+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:44.341+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.380+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:44.383+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:44.386+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:44.387+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:44.388+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:44.390+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:44.392+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:44.394+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:44.406+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:44.418+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:44.419+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:44.420+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:44.421+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:44.424+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:44.426+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:44.427+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:44.428+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:44.432+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:44.435+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:44.436+0000] {spark_submit.py:492} INFO - ... 1 more
[2025-10-17T07:27:44.442+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:44.497+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:44.502+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:44.508+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:44.514+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:44.516+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:44.517+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:44.518+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:44.522+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:44.523+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:44.524+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:44.525+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:44.531+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:44.533+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:44.533+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:44.534+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:44.535+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.538+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.539+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:44.540+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:44.540+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T07:27:44.541+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:27:44.546+0000] {spark_submit.py:492} INFO - Traceback (most recent call last):
[2025-10-17T07:27:44.548+0000] {spark_submit.py:492} INFO - File "/opt/***/dags/hdfs_spark_test.py", line 20, in main
[2025-10-17T07:27:44.549+0000] {spark_submit.py:492} INFO - df = spark.read.csv(hdfs_path, header=True, inferSchema=True)
[2025-10-17T07:27:44.550+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 740, in csv
[2025-10-17T07:27:44.553+0000] {spark_submit.py:492} INFO - return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
[2025-10-17T07:27:44.554+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-10-17T07:27:44.555+0000] {spark_submit.py:492} INFO - return_value = get_return_value(
[2025-10-17T07:27:44.557+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 179, in deco
[2025-10-17T07:27:44.558+0000] {spark_submit.py:492} INFO - return f(*a, **kw)
[2025-10-17T07:27:44.560+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py", line 326, in get_return_value
[2025-10-17T07:27:44.562+0000] {spark_submit.py:492} INFO - raise Py4JJavaError(
[2025-10-17T07:27:44.563+0000] {spark_submit.py:492} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o29.csv.
[2025-10-17T07:27:44.564+0000] {spark_submit.py:492} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (673fe1495c96 executor driver): org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv. Details:
[2025-10-17T07:27:44.565+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T07:27:44.566+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T07:27:44.567+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:44.567+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.568+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:44.569+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:44.570+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:44.571+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:44.572+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:44.573+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:44.574+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:44.576+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:44.577+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:44.578+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:44.579+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:44.580+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:44.581+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:44.581+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:44.583+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:44.584+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:44.585+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:44.586+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:44.586+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:44.588+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:44.589+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:44.591+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:44.594+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:44.595+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:44.596+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:44.597+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:44.598+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:44.599+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:44.600+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:44.601+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:44.602+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:44.603+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:44.604+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:44.606+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:44.607+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:44.609+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:44.611+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.612+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.612+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:44.613+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:44.614+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T07:27:44.615+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:27:44.616+0000] {spark_submit.py:492} INFO - Driver stacktrace:
[2025-10-17T07:27:44.617+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-10-17T07:27:44.618+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-10-17T07:27:44.618+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-10-17T07:27:44.620+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-10-17T07:27:44.622+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-10-17T07:27:44.623+0000] {spark_submit.py:492} INFO - at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-10-17T07:27:44.624+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-10-17T07:27:44.625+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-10-17T07:27:44.626+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-10-17T07:27:44.627+0000] {spark_submit.py:492} INFO - at scala.Option.foreach(Option.scala:407)
[2025-10-17T07:27:44.628+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-10-17T07:27:44.629+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-10-17T07:27:44.630+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-10-17T07:27:44.631+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-10-17T07:27:44.632+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-10-17T07:27:44.632+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-10-17T07:27:44.634+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)
[2025-10-17T07:27:44.635+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)
[2025-10-17T07:27:44.639+0000] {spark_submit.py:492} INFO - at org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)
[2025-10-17T07:27:44.641+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
[2025-10-17T07:27:44.642+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
[2025-10-17T07:27:44.643+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
[2025-10-17T07:27:44.644+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)
[2025-10-17T07:27:44.645+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)
[2025-10-17T07:27:44.646+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)
[2025-10-17T07:27:44.647+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)
[2025-10-17T07:27:44.648+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)
[2025-10-17T07:27:44.649+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-10-17T07:27:44.650+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-10-17T07:27:44.651+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-10-17T07:27:44.652+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-10-17T07:27:44.653+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-10-17T07:27:44.654+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)
[2025-10-17T07:27:44.655+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.head(Dataset.scala:3314)
[2025-10-17T07:27:44.657+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.Dataset.take(Dataset.scala:3537)
[2025-10-17T07:27:44.658+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.TextInputCSVDataSource$.infer(CSVDataSource.scala:111)
[2025-10-17T07:27:44.659+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVDataSource.inferSchema(CSVDataSource.scala:64)
[2025-10-17T07:27:44.660+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.inferSchema(CSVFileFormat.scala:62)
[2025-10-17T07:27:44.662+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$getOrInferFileFormatSchema$11(DataSource.scala:208)
[2025-10-17T07:27:44.663+0000] {spark_submit.py:492} INFO - at scala.Option.orElse(Option.scala:447)
[2025-10-17T07:27:44.664+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:205)
[2025-10-17T07:27:44.665+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:407)
[2025-10-17T07:27:44.668+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)
[2025-10-17T07:27:44.669+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)
[2025-10-17T07:27:44.676+0000] {spark_submit.py:492} INFO - at scala.Option.getOrElse(Option.scala:189)
[2025-10-17T07:27:44.678+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)
[2025-10-17T07:27:44.679+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:538)
[2025-10-17T07:27:44.680+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-10-17T07:27:44.681+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)
[2025-10-17T07:27:44.687+0000] {spark_submit.py:492} INFO - at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[2025-10-17T07:27:44.694+0000] {spark_submit.py:492} INFO - at java.base/java.lang.reflect.Method.invoke(Method.java:569)
[2025-10-17T07:27:44.695+0000] {spark_submit.py:492} INFO - at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-10-17T07:27:44.696+0000] {spark_submit.py:492} INFO - at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-10-17T07:27:44.703+0000] {spark_submit.py:492} INFO - at py4j.Gateway.invoke(Gateway.java:282)
[2025-10-17T07:27:44.705+0000] {spark_submit.py:492} INFO - at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-10-17T07:27:44.707+0000] {spark_submit.py:492} INFO - at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-10-17T07:27:44.708+0000] {spark_submit.py:492} INFO - at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-10-17T07:27:44.709+0000] {spark_submit.py:492} INFO - at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-10-17T07:27:44.710+0000] {spark_submit.py:492} INFO - at java.base/java.lang.Thread.run(Thread.java:840)
[2025-10-17T07:27:44.711+0000] {spark_submit.py:492} INFO - Caused by: org.apache.spark.SparkException: Encountered error while reading file hdfs://host.docker.internal:9000/user/winardi/input/olist_customers_dataset.csv. Details:
[2025-10-17T07:27:44.712+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)
[2025-10-17T07:27:44.715+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)
[2025-10-17T07:27:44.716+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:44.717+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.719+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
[2025-10-17T07:27:44.721+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
[2025-10-17T07:27:44.726+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)
[2025-10-17T07:27:44.728+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
[2025-10-17T07:27:44.729+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-10-17T07:27:44.731+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-10-17T07:27:44.732+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-10-17T07:27:44.733+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-10-17T07:27:44.733+0000] {spark_submit.py:492} INFO - at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-10-17T07:27:44.734+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-10-17T07:27:44.735+0000] {spark_submit.py:492} INFO - at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-10-17T07:27:44.736+0000] {spark_submit.py:492} INFO - at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-10-17T07:27:44.737+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-10-17T07:27:44.740+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-10-17T07:27:44.742+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-10-17T07:27:44.746+0000] {spark_submit.py:492} INFO - at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-10-17T07:27:44.747+0000] {spark_submit.py:492} INFO - at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-10-17T07:27:44.747+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)
[2025-10-17T07:27:44.748+0000] {spark_submit.py:492} INFO - at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)
[2025-10-17T07:27:44.750+0000] {spark_submit.py:492} INFO - ... 1 more
[2025-10-17T07:27:44.751+0000] {spark_submit.py:492} INFO - Caused by: org.apache.hadoop.hdfs.BlockMissingException: Could not obtain block: BP-611588423-192.168.56.1-1758120710649:blk_1073741826_1002 file=/user/winardi/input/olist_customers_dataset.csv No live nodes contain current block Block locations: DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK] Dead nodes:  DatanodeInfoWithStorage[127.0.0.1:9866,DS-51346003-6fcb-48ca-91d7-8e406910dfd0,DISK]
[2025-10-17T07:27:44.751+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.refetchLocations(DFSInputStream.java:1007)
[2025-10-17T07:27:44.752+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:990)
[2025-10-17T07:27:44.753+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.chooseDataNode(DFSInputStream.java:969)
[2025-10-17T07:27:44.754+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.blockSeekTo(DFSInputStream.java:677)
[2025-10-17T07:27:44.755+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:884)
[2025-10-17T07:27:44.756+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:957)
[2025-10-17T07:27:44.762+0000] {spark_submit.py:492} INFO - at java.base/java.io.DataInputStream.read(DataInputStream.java:151)
[2025-10-17T07:27:44.763+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.fillBuffer(UncompressedSplitLineReader.java:62)
[2025-10-17T07:27:44.782+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:227)
[2025-10-17T07:27:44.783+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.util.LineReader.readLine(LineReader.java:185)
[2025-10-17T07:27:44.784+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.UncompressedSplitLineReader.readLine(UncompressedSplitLineReader.java:94)
[2025-10-17T07:27:44.785+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.skipUtfByteOrderMark(LineRecordReader.java:158)
[2025-10-17T07:27:44.786+0000] {spark_submit.py:492} INFO - at org.apache.hadoop.mapreduce.lib.input.LineRecordReader.nextKeyValue(LineRecordReader.java:198)
[2025-10-17T07:27:44.787+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
[2025-10-17T07:27:44.788+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.HadoopFileLinesReader.hasNext(HadoopFileLinesReader.scala:67)
[2025-10-17T07:27:44.790+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.791+0000] {spark_submit.py:492} INFO - at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
[2025-10-17T07:27:44.792+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)
[2025-10-17T07:27:44.795+0000] {spark_submit.py:492} INFO - at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)
[2025-10-17T07:27:44.796+0000] {spark_submit.py:492} INFO - ... 22 more
[2025-10-17T07:27:44.797+0000] {spark_submit.py:492} INFO - 
[2025-10-17T07:27:44.798+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-10-17T07:27:44.799+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO SparkUI: Stopped Spark web UI at http://673fe1495c96:4040
[2025-10-17T07:27:44.800+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-10-17T07:27:44.800+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO MemoryStore: MemoryStore cleared
[2025-10-17T07:27:44.801+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO BlockManager: BlockManager stopped
[2025-10-17T07:27:44.802+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-10-17T07:27:44.803+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-10-17T07:27:44.808+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO SparkContext: Successfully stopped SparkContext
[2025-10-17T07:27:44.888+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO ShutdownHookManager: Shutdown hook called
[2025-10-17T07:27:44.890+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b984866-e8c8-476d-a1b4-936255869f1f
[2025-10-17T07:27:44.920+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:44 INFO ShutdownHookManager: Deleting directory /tmp/spark-6b984866-e8c8-476d-a1b4-936255869f1f/pyspark-5e4e868c-6fa9-46cb-8949-09c66b592327
[2025-10-17T07:27:45.014+0000] {spark_submit.py:492} INFO - 25/10/17 07:27:45 INFO ShutdownHookManager: Deleting directory /tmp/spark-a70d17cc-bd60-4165-806d-5f5d6d788dd3
[2025-10-17T07:27:45.214+0000] {taskinstance.py:2698} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/models/taskinstance.py", line 433, in _execute_task
    result = execute_callable(context=context, **execute_callable_kwargs)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/apache/spark/operators/spark_submit.py", line 157, in execute
    self._hook.submit(self._application)
  File "/home/airflow/.local/lib/python3.8/site-packages/airflow/providers/apache/spark/hooks/spark_submit.py", line 423, in submit
    raise AirflowException(
airflow.exceptions.AirflowException: Cannot execute: spark-submit --master local --name arrow-spark --verbose --queue default /opt/***/dags/hdfs_spark_test.py. Error code is: 1.
[2025-10-17T07:27:45.222+0000] {taskinstance.py:1138} INFO - Marking task as FAILED. dag_id=test_hdfs_spark, task_id=hdfs_spark_test, execution_date=20251017T072450, start_date=20251017T072521, end_date=20251017T072745
[2025-10-17T07:27:45.271+0000] {standard_task_runner.py:107} ERROR - Failed to execute job 54 for task hdfs_spark_test (Cannot execute: spark-submit --master local --name arrow-spark --verbose --queue default /opt/***/dags/hdfs_spark_test.py. Error code is: 1.; 1177)
[2025-10-17T07:27:45.321+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 1
[2025-10-17T07:27:45.384+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
