[2025-10-24T10:49:39.865+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: load_csv_to_postgres_dag.spark_load_csv_to_postgres manual__2025-10-24T10:49:27.418763+00:00 [queued]>
[2025-10-24T10:49:40.007+0000] {taskinstance.py:1956} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: load_csv_to_postgres_dag.spark_load_csv_to_postgres manual__2025-10-24T10:49:27.418763+00:00 [queued]>
[2025-10-24T10:49:40.018+0000] {taskinstance.py:2170} INFO - Starting attempt 1 of 1
[2025-10-24T10:49:40.106+0000] {taskinstance.py:2191} INFO - Executing <Task(SparkSubmitOperator): spark_load_csv_to_postgres> on 2025-10-24 10:49:27.418763+00:00
[2025-10-24T10:49:40.127+0000] {standard_task_runner.py:60} INFO - Started process 393 to run task
[2025-10-24T10:49:40.152+0000] {standard_task_runner.py:87} INFO - Running: ['***', 'tasks', 'run', 'load_csv_to_postgres_dag', 'spark_load_csv_to_postgres', 'manual__2025-10-24T10:49:27.418763+00:00', '--job-id', '199', '--raw', '--subdir', 'DAGS_FOLDER/dag_csv_to_postgres_dag.py', '--cfg-path', '/tmp/tmp9rdg81u6']
[2025-10-24T10:49:40.159+0000] {standard_task_runner.py:88} INFO - Job 199: Subtask spark_load_csv_to_postgres
[2025-10-24T10:49:40.647+0000] {task_command.py:423} INFO - Running <TaskInstance: load_csv_to_postgres_dag.spark_load_csv_to_postgres manual__2025-10-24T10:49:27.418763+00:00 [running]> on host 2ef039423968
[2025-10-24T10:49:41.329+0000] {taskinstance.py:2480} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='load_csv_to_postgres_dag' AIRFLOW_CTX_TASK_ID='spark_load_csv_to_postgres' AIRFLOW_CTX_EXECUTION_DATE='2025-10-24T10:49:27.418763+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-10-24T10:49:27.418763+00:00'
[2025-10-24T10:49:41.460+0000] {base.py:83} INFO - Using connection ID 'spark_conn' for task execution.
[2025-10-24T10:49:41.482+0000] {spark_submit.py:341} INFO - Spark-Submit cmd: spark-submit --master local --name arrow-spark --verbose /opt/***/include/scripts/load_csv_to_postgres.py
[2025-10-24T10:50:17.449+0000] {spark_submit.py:492} INFO - Using properties file: null
[2025-10-24T10:50:18.326+0000] {spark_submit.py:492} INFO - Parsed arguments:
[2025-10-24T10:50:18.398+0000] {spark_submit.py:492} INFO - master                  local
[2025-10-24T10:50:18.408+0000] {spark_submit.py:492} INFO - remote                  null
[2025-10-24T10:50:18.425+0000] {spark_submit.py:492} INFO - deployMode              null
[2025-10-24T10:50:18.441+0000] {spark_submit.py:492} INFO - executorMemory          null
[2025-10-24T10:50:18.511+0000] {spark_submit.py:492} INFO - executorCores           null
[2025-10-24T10:50:18.512+0000] {spark_submit.py:492} INFO - totalExecutorCores      null
[2025-10-24T10:50:18.513+0000] {spark_submit.py:492} INFO - propertiesFile          null
[2025-10-24T10:50:18.526+0000] {spark_submit.py:492} INFO - driverMemory            null
[2025-10-24T10:50:18.527+0000] {spark_submit.py:492} INFO - driverCores             null
[2025-10-24T10:50:18.529+0000] {spark_submit.py:492} INFO - driverExtraClassPath    null
[2025-10-24T10:50:18.532+0000] {spark_submit.py:492} INFO - driverExtraLibraryPath  null
[2025-10-24T10:50:18.533+0000] {spark_submit.py:492} INFO - driverExtraJavaOptions  null
[2025-10-24T10:50:18.537+0000] {spark_submit.py:492} INFO - supervise               false
[2025-10-24T10:50:18.538+0000] {spark_submit.py:492} INFO - queue                   null
[2025-10-24T10:50:18.539+0000] {spark_submit.py:492} INFO - numExecutors            null
[2025-10-24T10:50:18.541+0000] {spark_submit.py:492} INFO - files                   null
[2025-10-24T10:50:18.542+0000] {spark_submit.py:492} INFO - pyFiles                 null
[2025-10-24T10:50:18.544+0000] {spark_submit.py:492} INFO - archives                null
[2025-10-24T10:50:18.545+0000] {spark_submit.py:492} INFO - mainClass               null
[2025-10-24T10:50:18.612+0000] {spark_submit.py:492} INFO - primaryResource         file:/opt/***/include/scripts/load_csv_to_postgres.py
[2025-10-24T10:50:18.614+0000] {spark_submit.py:492} INFO - name                    arrow-spark
[2025-10-24T10:50:18.615+0000] {spark_submit.py:492} INFO - childArgs               []
[2025-10-24T10:50:18.616+0000] {spark_submit.py:492} INFO - jars                    null
[2025-10-24T10:50:18.619+0000] {spark_submit.py:492} INFO - packages                null
[2025-10-24T10:50:18.620+0000] {spark_submit.py:492} INFO - packagesExclusions      null
[2025-10-24T10:50:18.621+0000] {spark_submit.py:492} INFO - repositories            null
[2025-10-24T10:50:18.637+0000] {spark_submit.py:492} INFO - verbose                 true
[2025-10-24T10:50:18.638+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:50:18.639+0000] {spark_submit.py:492} INFO - Spark properties used, including those specified through
[2025-10-24T10:50:18.640+0000] {spark_submit.py:492} INFO - --conf and those from the properties file null:
[2025-10-24T10:50:18.641+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:50:18.642+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:50:18.704+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:50:21.137+0000] {spark_submit.py:492} INFO - Main class:
[2025-10-24T10:50:21.221+0000] {spark_submit.py:492} INFO - org.apache.spark.deploy.PythonRunner
[2025-10-24T10:50:21.233+0000] {spark_submit.py:492} INFO - Arguments:
[2025-10-24T10:50:21.294+0000] {spark_submit.py:492} INFO - file:/opt/***/include/scripts/load_csv_to_postgres.py
[2025-10-24T10:50:21.310+0000] {spark_submit.py:492} INFO - null
[2025-10-24T10:50:21.322+0000] {spark_submit.py:492} INFO - Spark config:
[2025-10-24T10:50:21.328+0000] {spark_submit.py:492} INFO - (spark.app.name,arrow-spark)
[2025-10-24T10:50:21.329+0000] {spark_submit.py:492} INFO - (spark.app.submitTime,1761303020972)
[2025-10-24T10:50:21.332+0000] {spark_submit.py:492} INFO - (spark.master,local)
[2025-10-24T10:50:21.334+0000] {spark_submit.py:492} INFO - (spark.submit.deployMode,client)
[2025-10-24T10:50:21.335+0000] {spark_submit.py:492} INFO - (spark.submit.pyFiles,)
[2025-10-24T10:50:21.336+0000] {spark_submit.py:492} INFO - Classpath elements:
[2025-10-24T10:50:21.337+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:50:21.347+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:50:21.348+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:50:28.341+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:28 INFO SparkContext: Running Spark version 3.5.1
[2025-10-24T10:50:28.372+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:28 INFO SparkContext: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-10-24T10:50:28.386+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:28 INFO SparkContext: Java version 17.0.16
[2025-10-24T10:50:30.022+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-10-24T10:50:31.594+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:31 INFO ResourceUtils: ==============================================================
[2025-10-24T10:50:31.597+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:31 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-10-24T10:50:31.610+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:31 INFO ResourceUtils: ==============================================================
[2025-10-24T10:50:31.611+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:31 INFO SparkContext: Submitted application: Load CSV to Postgres
[2025-10-24T10:50:31.841+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:31 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-10-24T10:50:31.941+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:31 INFO ResourceProfile: Limiting resource is cpu
[2025-10-24T10:50:31.944+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:31 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-10-24T10:50:32.603+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:32 INFO SecurityManager: Changing view acls to: ***
[2025-10-24T10:50:32.604+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:32 INFO SecurityManager: Changing modify acls to: ***
[2025-10-24T10:50:32.624+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:32 INFO SecurityManager: Changing view acls groups to:
[2025-10-24T10:50:32.797+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:32 INFO SecurityManager: Changing modify acls groups to:
[2025-10-24T10:50:32.934+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:32 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-10-24T10:50:41.012+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:41 INFO Utils: Successfully started service 'sparkDriver' on port 45387.
[2025-10-24T10:50:41.551+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:41 INFO SparkEnv: Registering MapOutputTracker
[2025-10-24T10:50:42.431+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:42 INFO SparkEnv: Registering BlockManagerMaster
[2025-10-24T10:50:42.729+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-10-24T10:50:42.731+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-10-24T10:50:42.747+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:42 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-10-24T10:50:43.034+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-60ad9085-7c08-4be4-bcb7-14a8ddefb2af
[2025-10-24T10:50:43.207+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:43 INFO MemoryStore: MemoryStore started with capacity 413.9 MiB
[2025-10-24T10:50:43.497+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:43 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-10-24T10:50:47.211+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:47 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-10-24T10:50:48.639+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-10-24T10:50:49.195+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:49 INFO SparkContext: Added JAR /opt/spark/jars/postgresql-42.6.0.jar at spark://2ef039423968:45387/jars/postgresql-42.6.0.jar with timestamp 1761303028169
[2025-10-24T10:50:50.629+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:50 INFO Executor: Starting executor ID driver on host 2ef039423968
[2025-10-24T10:50:50.699+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:50 INFO Executor: OS info Linux, 6.6.87.2-microsoft-standard-WSL2, amd64
[2025-10-24T10:50:50.707+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:50 INFO Executor: Java version 17.0.16
[2025-10-24T10:50:50.829+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:50 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''
[2025-10-24T10:50:50.895+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:50 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1d5b87d6 for default.
[2025-10-24T10:50:50.921+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:50 INFO Executor: Fetching spark://2ef039423968:45387/jars/postgresql-42.6.0.jar with timestamp 1761303028169
[2025-10-24T10:50:51.532+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:51 INFO TransportClientFactory: Successfully created connection to 2ef039423968/172.18.0.4:45387 after 390 ms (0 ms spent in bootstraps)
[2025-10-24T10:50:51.632+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:51 INFO Utils: Fetching spark://2ef039423968:45387/jars/postgresql-42.6.0.jar to /tmp/spark-5afbf267-c8c4-4e41-af4e-cd1801b81250/userFiles-f4c09068-a10f-4a18-87c5-fd7716e80cd8/fetchFileTemp18251156315544180833.tmp
[2025-10-24T10:50:52.111+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:52 INFO Executor: Adding file:/tmp/spark-5afbf267-c8c4-4e41-af4e-cd1801b81250/userFiles-f4c09068-a10f-4a18-87c5-fd7716e80cd8/postgresql-42.6.0.jar to class loader default
[2025-10-24T10:50:52.197+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:52 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33415.
[2025-10-24T10:50:52.198+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:52 INFO NettyBlockTransferService: Server created on 2ef039423968:33415
[2025-10-24T10:50:52.203+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:52 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-10-24T10:50:52.225+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:52 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 2ef039423968, 33415, None)
[2025-10-24T10:50:52.233+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:52 INFO BlockManagerMasterEndpoint: Registering block manager 2ef039423968:33415 with 413.9 MiB RAM, BlockManagerId(driver, 2ef039423968, 33415, None)
[2025-10-24T10:50:52.239+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:52 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 2ef039423968, 33415, None)
[2025-10-24T10:50:52.241+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:52 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 2ef039423968, 33415, None)
[2025-10-24T10:50:56.039+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:56 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-10-24T10:50:56.099+0000] {spark_submit.py:492} INFO - 25/10/24 10:50:56 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-10-24T10:51:12.122+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:12 INFO CodeGenerator: Code generated in 2400.356901 ms
[2025-10-24T10:51:13.219+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:13 INFO DAGScheduler: Registering RDD 2 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2025-10-24T10:51:13.402+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:13 INFO DAGScheduler: Got map stage job 0 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-24T10:51:13.410+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:13 INFO DAGScheduler: Final stage: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0)
[2025-10-24T10:51:13.413+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:13 INFO DAGScheduler: Parents of final stage: List()
[2025-10-24T10:51:13.424+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:13 INFO DAGScheduler: Missing parents: List()
[2025-10-24T10:51:13.513+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:13 INFO DAGScheduler: Submitting ShuffleMapStage 0 (MapPartitionsRDD[2] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-24T10:51:15.319+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:15 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.8 KiB, free 413.9 MiB)
[2025-10-24T10:51:15.712+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:15 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 7.2 KiB, free 413.9 MiB)
[2025-10-24T10:51:15.719+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:15 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 2ef039423968:33415 (size: 7.2 KiB, free: 413.9 MiB)
[2025-10-24T10:51:15.799+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:15 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1585
[2025-10-24T10:51:16.005+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:16 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 0 (MapPartitionsRDD[2] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-24T10:51:16.022+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:16 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-10-24T10:51:16.597+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:16 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (2ef039423968, executor driver, partition 0, PROCESS_LOCAL, 7630 bytes)
[2025-10-24T10:51:16.696+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:16 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
[2025-10-24T10:51:18.014+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO CodeGenerator: Code generated in 168.301618 ms
[2025-10-24T10:51:18.124+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO JDBCRDD: closed connection
[2025-10-24T10:51:18.595+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1928 bytes result sent to driver
[2025-10-24T10:51:18.708+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2208 ms on 2ef039423968 (executor driver) (1/1)
[2025-10-24T10:51:18.714+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-10-24T10:51:18.805+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO DAGScheduler: ShuffleMapStage 0 (count at NativeMethodAccessorImpl.java:0) finished in 5.088 s
[2025-10-24T10:51:18.911+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO DAGScheduler: looking for newly runnable stages
[2025-10-24T10:51:18.915+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO DAGScheduler: running: Set()
[2025-10-24T10:51:18.996+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO DAGScheduler: waiting: Set()
[2025-10-24T10:51:19.015+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:18 INFO DAGScheduler: failed: Set()
[2025-10-24T10:51:19.616+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:19 INFO CodeGenerator: Code generated in 203.939522 ms
[2025-10-24T10:51:20.395+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:20.495+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO DAGScheduler: Got job 1 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-24T10:51:20.603+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO DAGScheduler: Final stage: ResultStage 2 (count at NativeMethodAccessorImpl.java:0)
[2025-10-24T10:51:20.610+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 1)
[2025-10-24T10:51:20.627+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO DAGScheduler: Missing parents: List()
[2025-10-24T10:51:20.695+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-24T10:51:20.705+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 12.5 KiB, free 413.9 MiB)
[2025-10-24T10:51:20.711+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 413.9 MiB)
[2025-10-24T10:51:20.712+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 2ef039423968:33415 (size: 5.9 KiB, free: 413.9 MiB)
[2025-10-24T10:51:20.714+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585
[2025-10-24T10:51:20.718+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[5] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-24T10:51:20.719+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks resource profile 0
[2025-10-24T10:51:20.824+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 1) (2ef039423968, executor driver, partition 0, NODE_LOCAL, 7795 bytes)
[2025-10-24T10:51:20.833+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:20 INFO Executor: Running task 0.0 in stage 2.0 (TID 1)
[2025-10-24T10:51:21.423+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-24T10:51:21.512+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 179 ms
[2025-10-24T10:51:21.707+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO CodeGenerator: Code generated in 162.395988 ms
[2025-10-24T10:51:21.816+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO Executor: Finished task 0.0 in stage 2.0 (TID 1). 4038 bytes result sent to driver
[2025-10-24T10:51:21.897+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 1) in 1016 ms on 2ef039423968 (executor driver) (1/1)
[2025-10-24T10:51:21.900+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-10-24T10:51:21.906+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO DAGScheduler: ResultStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 1.207 s
[2025-10-24T10:51:21.915+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-24T10:51:21.925+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO TaskSchedulerImpl: Killing all running tasks in stage 2: Stage finished
[2025-10-24T10:51:21.931+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:21 INFO DAGScheduler: Job 1 finished: count at NativeMethodAccessorImpl.java:0, took 1.578892 s
[2025-10-24T10:51:22.021+0000] {spark_submit.py:492} INFO - ✅ PostgreSQL connection successful
[2025-10-24T10:51:22.638+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 2ef039423968:33415 in memory (size: 5.9 KiB, free: 413.9 MiB)
[2025-10-24T10:51:25.014+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:25 INFO InMemoryFileIndex: It took 107 ms to list leaf files for 1 paths.
[2025-10-24T10:51:26.124+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:26 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 2ef039423968:33415 in memory (size: 7.2 KiB, free: 413.9 MiB)
[2025-10-24T10:51:26.332+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:26 INFO InMemoryFileIndex: It took 212 ms to list leaf files for 1 paths.
[2025-10-24T10:51:38.149+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO FileSourceStrategy: Pushed Filters:
[2025-10-24T10:51:38.196+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#8, None)) > 0)
[2025-10-24T10:51:38.419+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO CodeGenerator: Code generated in 97.455215 ms
[2025-10-24T10:51:38.603+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 200.0 KiB, free 413.7 MiB)
[2025-10-24T10:51:38.717+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 413.7 MiB)
[2025-10-24T10:51:38.719+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 2ef039423968:33415 (size: 34.5 KiB, free: 413.9 MiB)
[2025-10-24T10:51:38.721+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:38.801+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO FileSourceScanExec: Planning scan with bin packing, max size: 49020463 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-24T10:51:38.913+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:38.915+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO DAGScheduler: Got job 2 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-24T10:51:38.916+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO DAGScheduler: Final stage: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0)
[2025-10-24T10:51:38.918+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO DAGScheduler: Parents of final stage: List()
[2025-10-24T10:51:38.919+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO DAGScheduler: Missing parents: List()
[2025-10-24T10:51:38.921+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-24T10:51:38.997+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:38 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 13.5 KiB, free 413.7 MiB)
[2025-10-24T10:51:39.006+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 413.7 MiB)
[2025-10-24T10:51:39.009+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 2ef039423968:33415 (size: 6.4 KiB, free: 413.9 MiB)
[2025-10-24T10:51:39.011+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1585
[2025-10-24T10:51:39.014+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-24T10:51:39.015+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0
[2025-10-24T10:51:39.027+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (2ef039423968, executor driver, partition 0, PROCESS_LOCAL, 8412 bytes)
[2025-10-24T10:51:39.030+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)
[2025-10-24T10:51:39.226+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO CodeGenerator: Code generated in 82.471343 ms
[2025-10-24T10:51:39.296+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO FileScanRDD: Reading File path: file:///opt/***/include/dfTransjakarta180kRows.csv, range: 0-44826159, partition values: [empty row]
[2025-10-24T10:51:39.429+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO CodeGenerator: Code generated in 130.414891 ms
[2025-10-24T10:51:39.810+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 1743 bytes result sent to driver
[2025-10-24T10:51:39.843+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 823 ms on 2ef039423968 (executor driver) (1/1)
[2025-10-24T10:51:39.860+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool
[2025-10-24T10:51:39.865+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO DAGScheduler: ResultStage 3 (csv at NativeMethodAccessorImpl.java:0) finished in 0.918 s
[2025-10-24T10:51:39.868+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-24T10:51:39.870+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished
[2025-10-24T10:51:39.872+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO DAGScheduler: Job 2 finished: csv at NativeMethodAccessorImpl.java:0, took 0.934777 s
[2025-10-24T10:51:39.958+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:39 INFO CodeGenerator: Code generated in 49.020761 ms
[2025-10-24T10:51:40.199+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 2ef039423968:33415 in memory (size: 6.4 KiB, free: 413.9 MiB)
[2025-10-24T10:51:40.313+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO FileSourceStrategy: Pushed Filters:
[2025-10-24T10:51:40.316+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-24T10:51:40.326+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 200.0 KiB, free 413.5 MiB)
[2025-10-24T10:51:40.407+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 413.5 MiB)
[2025-10-24T10:51:40.415+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 2ef039423968:33415 (size: 34.5 KiB, free: 413.9 MiB)
[2025-10-24T10:51:40.419+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO SparkContext: Created broadcast 4 from csv at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:40.429+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO FileSourceScanExec: Planning scan with bin packing, max size: 49020463 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-24T10:51:40.839+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:40.901+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO DAGScheduler: Got job 3 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-24T10:51:40.906+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO DAGScheduler: Final stage: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0)
[2025-10-24T10:51:40.915+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO DAGScheduler: Parents of final stage: List()
[2025-10-24T10:51:40.916+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO DAGScheduler: Missing parents: List()
[2025-10-24T10:51:40.917+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:40 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-24T10:51:41.014+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 28.0 KiB, free 413.4 MiB)
[2025-10-24T10:51:41.020+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 413.4 MiB)
[2025-10-24T10:51:41.021+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 2ef039423968:33415 (size: 12.9 KiB, free: 413.8 MiB)
[2025-10-24T10:51:41.023+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1585
[2025-10-24T10:51:41.025+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[15] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-24T10:51:41.026+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-10-24T10:51:41.028+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (2ef039423968, executor driver, partition 0, PROCESS_LOCAL, 8412 bytes)
[2025-10-24T10:51:41.096+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)
[2025-10-24T10:51:41.298+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO CodeGenerator: Code generated in 82.544444 ms
[2025-10-24T10:51:41.307+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:41 INFO FileScanRDD: Reading File path: file:///opt/***/include/dfTransjakarta180kRows.csv, range: 0-44826159, partition values: [empty row]
[2025-10-24T10:51:49.896+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:49 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 1908 bytes result sent to driver
[2025-10-24T10:51:49.908+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:49 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 8872 ms on 2ef039423968 (executor driver) (1/1)
[2025-10-24T10:51:49.999+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:49 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-10-24T10:51:50.003+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:49 INFO DAGScheduler: ResultStage 4 (csv at NativeMethodAccessorImpl.java:0) finished in 8.998 s
[2025-10-24T10:51:50.010+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:49 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-24T10:51:50.012+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-10-24T10:51:50.020+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:49 INFO DAGScheduler: Job 3 finished: csv at NativeMethodAccessorImpl.java:0, took 9.072479 s
[2025-10-24T10:51:50.119+0000] {spark_submit.py:492} INFO - ✅ CSV loaded successfully
[2025-10-24T10:51:50.516+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:50 INFO FileSourceStrategy: Pushed Filters:
[2025-10-24T10:51:50.524+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:50 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-24T10:51:50.901+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:50 INFO CodeGenerator: Code generated in 109.395155 ms
[2025-10-24T10:51:50.918+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:50 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 200.0 KiB, free 413.2 MiB)
[2025-10-24T10:51:51.014+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 413.2 MiB)
[2025-10-24T10:51:51.015+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 2ef039423968:33415 (size: 34.4 KiB, free: 413.8 MiB)
[2025-10-24T10:51:51.016+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO SparkContext: Created broadcast 6 from count at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:51.195+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO FileSourceScanExec: Planning scan with bin packing, max size: 49020463 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-24T10:51:51.218+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO DAGScheduler: Registering RDD 19 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1
[2025-10-24T10:51:51.306+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO DAGScheduler: Got map stage job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-24T10:51:51.315+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO DAGScheduler: Final stage: ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0)
[2025-10-24T10:51:51.395+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO DAGScheduler: Parents of final stage: List()
[2025-10-24T10:51:51.411+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO DAGScheduler: Missing parents: List()
[2025-10-24T10:51:51.412+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-24T10:51:51.418+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 18.7 KiB, free 413.2 MiB)
[2025-10-24T10:51:51.498+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.2 KiB, free 413.2 MiB)
[2025-10-24T10:51:51.519+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 2ef039423968:33415 (size: 9.2 KiB, free: 413.8 MiB)
[2025-10-24T10:51:51.597+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1585
[2025-10-24T10:51:51.603+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[19] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-24T10:51:51.610+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-10-24T10:51:51.620+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (2ef039423968, executor driver, partition 0, PROCESS_LOCAL, 8401 bytes)
[2025-10-24T10:51:51.623+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO Executor: Running task 0.0 in stage 5.0 (TID 4)
[2025-10-24T10:51:51.624+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO CodeGenerator: Code generated in 103.568447 ms
[2025-10-24T10:51:51.695+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO FileScanRDD: Reading File path: file:///opt/***/include/dfTransjakarta180kRows.csv, range: 0-44826159, partition values: [empty row]
[2025-10-24T10:51:51.698+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:51 INFO CodeGenerator: Code generated in 17.282724 ms
[2025-10-24T10:51:54.912+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:54 INFO Executor: Finished task 0.0 in stage 5.0 (TID 4). 1971 bytes result sent to driver
[2025-10-24T10:51:54.915+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:54 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 3697 ms on 2ef039423968 (executor driver) (1/1)
[2025-10-24T10:51:54.917+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:54 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-10-24T10:51:54.920+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:54 INFO DAGScheduler: ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 3.806 s
[2025-10-24T10:51:54.923+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:54 INFO DAGScheduler: looking for newly runnable stages
[2025-10-24T10:51:54.924+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:54 INFO DAGScheduler: running: Set()
[2025-10-24T10:51:54.925+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:54 INFO DAGScheduler: waiting: Set()
[2025-10-24T10:51:54.998+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:54 INFO DAGScheduler: failed: Set()
[2025-10-24T10:51:55.126+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:55.195+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: Got job 5 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-24T10:51:55.198+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: Final stage: ResultStage 7 (count at NativeMethodAccessorImpl.java:0)
[2025-10-24T10:51:55.207+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 6)
[2025-10-24T10:51:55.208+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: Missing parents: List()
[2025-10-24T10:51:55.209+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-24T10:51:55.210+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 12.5 KiB, free 413.2 MiB)
[2025-10-24T10:51:55.213+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 5.9 KiB, free 413.2 MiB)
[2025-10-24T10:51:55.215+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 2ef039423968:33415 (size: 5.9 KiB, free: 413.8 MiB)
[2025-10-24T10:51:55.217+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1585
[2025-10-24T10:51:55.218+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-24T10:51:55.220+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-10-24T10:51:55.298+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (2ef039423968, executor driver, partition 0, NODE_LOCAL, 7795 bytes)
[2025-10-24T10:51:55.316+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)
[2025-10-24T10:51:55.415+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO ShuffleBlockFetcherIterator: Getting 1 (60.0 B) non-empty blocks including 1 (60.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks
[2025-10-24T10:51:55.417+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 4 ms
[2025-10-24T10:51:55.427+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 3995 bytes result sent to driver
[2025-10-24T10:51:55.507+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 284 ms on 2ef039423968 (executor driver) (1/1)
[2025-10-24T10:51:55.513+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-10-24T10:51:55.595+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: ResultStage 7 (count at NativeMethodAccessorImpl.java:0) finished in 0.312 s
[2025-10-24T10:51:55.611+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-24T10:51:55.615+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-10-24T10:51:55.695+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:55 INFO DAGScheduler: Job 5 finished: count at NativeMethodAccessorImpl.java:0, took 0.469537 s
[2025-10-24T10:51:55.698+0000] {spark_submit.py:492} INFO - Record count: 189500
[2025-10-24T10:51:56.197+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:56 INFO FileSourceStrategy: Pushed Filters:
[2025-10-24T10:51:56.206+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:56 INFO FileSourceStrategy: Post-Scan Filters:
[2025-10-24T10:51:57.101+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO CodeGenerator: Code generated in 484.047468 ms
[2025-10-24T10:51:57.114+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 200.0 KiB, free 413.0 MiB)
[2025-10-24T10:51:57.221+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 34.4 KiB, free 412.9 MiB)
[2025-10-24T10:51:57.234+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 2ef039423968:33415 (size: 34.4 KiB, free: 413.8 MiB)
[2025-10-24T10:51:57.303+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO SparkContext: Created broadcast 9 from showString at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:57.307+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO FileSourceScanExec: Planning scan with bin packing, max size: 49020463 bytes, open cost is considered as scanning 4194304 bytes.
[2025-10-24T10:51:57.399+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2025-10-24T10:51:57.497+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-10-24T10:51:57.515+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO DAGScheduler: Final stage: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0)
[2025-10-24T10:51:57.596+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO DAGScheduler: Parents of final stage: List()
[2025-10-24T10:51:57.607+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO DAGScheduler: Missing parents: List()
[2025-10-24T10:51:57.628+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-10-24T10:51:57.699+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 26.5 KiB, free 412.9 MiB)
[2025-10-24T10:51:57.710+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 10.1 KiB, free 412.9 MiB)
[2025-10-24T10:51:57.798+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 2ef039423968:33415 (size: 10.1 KiB, free: 413.8 MiB)
[2025-10-24T10:51:57.899+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 2ef039423968:33415 in memory (size: 5.9 KiB, free: 413.8 MiB)
[2025-10-24T10:51:57.912+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1585
[2025-10-24T10:51:57.913+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[26] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-10-24T10:51:57.914+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-10-24T10:51:57.995+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (2ef039423968, executor driver, partition 0, PROCESS_LOCAL, 8412 bytes)
[2025-10-24T10:51:58.015+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:57 INFO Executor: Running task 0.0 in stage 8.0 (TID 6)
[2025-10-24T10:51:58.208+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:58 INFO CodeGenerator: Code generated in 481.722476 ms
[2025-10-24T10:51:58.219+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:58 INFO FileScanRDD: Reading File path: file:///opt/***/include/dfTransjakarta180kRows.csv, range: 0-44826159, partition values: [empty row]
[2025-10-24T10:51:58.795+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:58 INFO CodeGenerator: Code generated in 295.298765 ms
[2025-10-24T10:51:59.105+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:59 INFO Executor: Finished task 0.0 in stage 8.0 (TID 6). 3577 bytes result sent to driver
[2025-10-24T10:51:59.123+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:59 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 1423 ms on 2ef039423968 (executor driver) (1/1)
[2025-10-24T10:51:59.131+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:59 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-10-24T10:51:59.196+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:59 INFO DAGScheduler: ResultStage 8 (showString at NativeMethodAccessorImpl.java:0) finished in 1.628 s
[2025-10-24T10:51:59.233+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:59 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-10-24T10:51:59.296+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:59 INFO TaskSchedulerImpl: Killing all running tasks in stage 8: Stage finished
[2025-10-24T10:51:59.332+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:59 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 1.871891 s
[2025-10-24T10:51:59.797+0000] {spark_submit.py:492} INFO - 25/10/24 10:51:59 INFO CodeGenerator: Code generated in 367.852636 ms
[2025-10-24T10:51:59.914+0000] {spark_submit.py:492} INFO - +--------------+----------------+-----------+--------------------+----------+----------------+----------+--------------------+---------+----------+--------------------+-------------+-------------+------------+-------------------+-----------+--------------------+--------------+--------------+----------+-------------------+---------+
[2025-10-24T10:52:00.014+0000] {spark_submit.py:492} INFO - |       transID|       payCardID|payCardBank|         payCardName|payCardSex|payCardBirthDate|corridorID|        corridorName|direction|tapInStops|      tapInStopsName|tapInStopsLat|tapInStopsLon|stopStartSeq|          tapInTime|tapOutStops|     tapOutStopsName|tapOutStopsLat|tapOutStopsLon|stopEndSeq|         tapOutTime|payAmount|
[2025-10-24T10:52:00.095+0000] {spark_submit.py:492} INFO - +--------------+----------------+-----------+--------------------+----------+----------------+----------+--------------------+---------+----------+--------------------+-------------+-------------+------------+-------------------+-----------+--------------------+--------------+--------------+----------+-------------------+---------+
[2025-10-24T10:52:00.098+0000] {spark_submit.py:492} INFO - |VRPJ892P3M98RA|3561407960318444|        dki|Dr. Janet Nashiru...|         M|            2010|         4|Pulo Gadung 2 - T...|      1.0|    P00167|   Pemuda Rawamangun|    -6.193488|    106.89165|          12|2023-04-03 06:53:02|     P00127|               Layur|     -6.193539|     106.89909|      13.0|2023-04-03 07:13:28|   3500.0|
[2025-10-24T10:52:00.107+0000] {spark_submit.py:492} INFO - |ZWCH834I6M26HS| 347728053419394|     emoney|   Balamantri Rahayu|         M|            2002|    JAK.28|Kp. Rambutan - Ta...|      1.0|   B04633P|Sekolah Islam PB ...|    -6.314293|    106.86298|          27|2023-04-03 05:59:19|    B00865P|       Jln. Ar Ridho|     -6.308148|     106.86935|      30.0|2023-04-03 06:57:06|      0.0|
[2025-10-24T10:52:00.108+0000] {spark_submit.py:492} INFO - |YRLD835V6L82GO| 377105453850671|     emoney|        Dian Mustofa|         F|            1993|       B13|Bekasi Barat - Bl...|      1.0|   B02192P|   Mall Metropolitan|    -6.247861|    106.99215|           6|2023-04-03 05:13:24|    B00108P|Bandar Djakarta B...|     -6.227085|     106.99683|       9.0|2023-04-03 06:01:23|  20000.0|
[2025-10-24T10:52:00.109+0000] {spark_submit.py:492} INFO - |ZZBX143N6N83HQ|4486493302356581|        dki|Cut Janet Suryatm...|         M|            1980|        8K|   Batusari - Grogol|      1.0|   B03637P|Sbr. Jln. Angsana...|    -6.194813|    106.78213|          16|2023-04-03 05:20:24|       NULL|Yayasan Alkahfi J...|     -6.198896|     106.76889|      26.0|2023-04-03 06:01:25|   3500.0|
[2025-10-24T10:52:00.115+0000] {spark_submit.py:492} INFO - |EWEG491A2W45DR|  30139379978125|        bni|dr. Mulyanto Pudj...|         F|            1997|      NULL|                NULL|      0.0|      NULL|         BRI Menteng|    -6.186792|    106.83514|           2|2023-04-03 06:00:54|    B02755P|Perpustakaan Nasi...|     -6.180673|     106.82643|       5.0|2023-04-03 06:47:32|   3500.0|
[2025-10-24T10:52:00.116+0000] {spark_submit.py:492} INFO - +--------------+----------------+-----------+--------------------+----------+----------------+----------+--------------------+---------+----------+--------------------+-------------+-------------+------------+-------------------+-----------+--------------------+--------------+--------------+----------+-------------------+---------+
[2025-10-24T10:52:00.117+0000] {spark_submit.py:492} INFO - only showing top 5 rows
[2025-10-24T10:52:00.125+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:52:01.528+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:01 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 2ef039423968:33415 in memory (size: 10.1 KiB, free: 413.8 MiB)
[2025-10-24T10:52:04.207+0000] {spark_submit.py:492} INFO - Schema after transformation:
[2025-10-24T10:52:04.225+0000] {spark_submit.py:492} INFO - root
[2025-10-24T10:52:04.232+0000] {spark_submit.py:492} INFO - |-- transID: string (nullable = true)
[2025-10-24T10:52:04.297+0000] {spark_submit.py:492} INFO - |-- payCardID: long (nullable = true)
[2025-10-24T10:52:04.307+0000] {spark_submit.py:492} INFO - |-- payCardBank: string (nullable = true)
[2025-10-24T10:52:04.309+0000] {spark_submit.py:492} INFO - |-- payCardName: string (nullable = true)
[2025-10-24T10:52:04.309+0000] {spark_submit.py:492} INFO - |-- payCardSex: string (nullable = true)
[2025-10-24T10:52:04.310+0000] {spark_submit.py:492} INFO - |-- payCardBirthDate: integer (nullable = true)
[2025-10-24T10:52:04.311+0000] {spark_submit.py:492} INFO - |-- corridorID: string (nullable = true)
[2025-10-24T10:52:04.311+0000] {spark_submit.py:492} INFO - |-- corridorName: string (nullable = true)
[2025-10-24T10:52:04.313+0000] {spark_submit.py:492} INFO - |-- direction: integer (nullable = true)
[2025-10-24T10:52:04.315+0000] {spark_submit.py:492} INFO - |-- tapInStops: string (nullable = true)
[2025-10-24T10:52:04.316+0000] {spark_submit.py:492} INFO - |-- tapInStopsName: string (nullable = true)
[2025-10-24T10:52:04.317+0000] {spark_submit.py:492} INFO - |-- tapInStopsLat: double (nullable = true)
[2025-10-24T10:52:04.318+0000] {spark_submit.py:492} INFO - |-- tapInStopsLon: double (nullable = true)
[2025-10-24T10:52:04.319+0000] {spark_submit.py:492} INFO - |-- stopStartSeq: integer (nullable = true)
[2025-10-24T10:52:04.320+0000] {spark_submit.py:492} INFO - |-- tapInTime: timestamp (nullable = true)
[2025-10-24T10:52:04.323+0000] {spark_submit.py:492} INFO - |-- tapOutStops: string (nullable = true)
[2025-10-24T10:52:04.324+0000] {spark_submit.py:492} INFO - |-- tapOutStopsName: string (nullable = true)
[2025-10-24T10:52:04.325+0000] {spark_submit.py:492} INFO - |-- tapOutStopsLat: double (nullable = true)
[2025-10-24T10:52:04.326+0000] {spark_submit.py:492} INFO - |-- tapOutStopsLon: double (nullable = true)
[2025-10-24T10:52:04.327+0000] {spark_submit.py:492} INFO - |-- stopEndSeq: integer (nullable = true)
[2025-10-24T10:52:04.395+0000] {spark_submit.py:492} INFO - |-- tapOutTime: timestamp (nullable = true)
[2025-10-24T10:52:04.397+0000] {spark_submit.py:492} INFO - |-- payAmount: double (nullable = true)
[2025-10-24T10:52:04.398+0000] {spark_submit.py:492} INFO - |-- load_timestamp: timestamp (nullable = false)
[2025-10-24T10:52:04.402+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:52:05.323+0000] {spark_submit.py:492} INFO - Error:
[2025-10-24T10:52:05.612+0000] {spark_submit.py:492} INFO - [PARSE_SYNTAX_ERROR] Syntax error at or near 'PRECISION'.(line 1, pos 223)
[2025-10-24T10:52:05.628+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:52:05.632+0000] {spark_submit.py:492} INFO - == SQL ==
[2025-10-24T10:52:05.643+0000] {spark_submit.py:492} INFO - transID TEXT, payCardID TEXT, payCardBank TEXT, payCardName TEXT, payCardSex TEXT, payCardBirthDate INTEGER, corridorID TEXT, corridorName TEXT, direction INTEGER, tapInStops TEXT, tapInStopsName TEXT, tapInStopsLat DOUBLE PRECISION, tapInStopsLon DOUBLE PRECISION, stopStartSeq INTEGER, tapInTime TIMESTAMP, tapOutStops TEXT, tapOutStopsName TEXT, tapOutStopsLat DOUBLE PRECISION, tapOutStopsLon DOUBLE PRECISION, stopEndSeq INTEGER, tapOutTime TIMESTAMP, payAmount DOUBLE PRECISION, load_timestamp TIMESTAMP
[2025-10-24T10:52:06.085+0000] {spark_submit.py:492} INFO - -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^
[2025-10-24T10:52:06.406+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:52:06.432+0000] {spark_submit.py:492} INFO - Traceback (most recent call last):
[2025-10-24T10:52:06.442+0000] {spark_submit.py:492} INFO - File "/opt/***/include/scripts/load_csv_to_postgres.py", line 78, in main
[2025-10-24T10:52:06.445+0000] {spark_submit.py:492} INFO - df_transformed.write \
[2025-10-24T10:52:06.456+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py", line 1984, in jdbc
[2025-10-24T10:52:06.457+0000] {spark_submit.py:492} INFO - self.mode(mode)._jwrite.jdbc(url, table, jprop)
[2025-10-24T10:52:06.459+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-10-24T10:52:06.460+0000] {spark_submit.py:492} INFO - return_value = get_return_value(
[2025-10-24T10:52:06.463+0000] {spark_submit.py:492} INFO - File "/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-10-24T10:52:06.471+0000] {spark_submit.py:492} INFO - raise converted from None
[2025-10-24T10:52:06.473+0000] {spark_submit.py:492} INFO - pyspark.errors.exceptions.captured.ParseException:
[2025-10-24T10:52:06.495+0000] {spark_submit.py:492} INFO - [PARSE_SYNTAX_ERROR] Syntax error at or near 'PRECISION'.(line 1, pos 223)
[2025-10-24T10:52:06.505+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:52:06.507+0000] {spark_submit.py:492} INFO - == SQL ==
[2025-10-24T10:52:06.508+0000] {spark_submit.py:492} INFO - transID TEXT, payCardID TEXT, payCardBank TEXT, payCardName TEXT, payCardSex TEXT, payCardBirthDate INTEGER, corridorID TEXT, corridorName TEXT, direction INTEGER, tapInStops TEXT, tapInStopsName TEXT, tapInStopsLat DOUBLE PRECISION, tapInStopsLon DOUBLE PRECISION, stopStartSeq INTEGER, tapInTime TIMESTAMP, tapOutStops TEXT, tapOutStopsName TEXT, tapOutStopsLat DOUBLE PRECISION, tapOutStopsLon DOUBLE PRECISION, stopEndSeq INTEGER, tapOutTime TIMESTAMP, payAmount DOUBLE PRECISION, load_timestamp TIMESTAMP
[2025-10-24T10:52:06.512+0000] {spark_submit.py:492} INFO - -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------^^^
[2025-10-24T10:52:06.526+0000] {spark_submit.py:492} INFO - 
[2025-10-24T10:52:06.529+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:06 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-10-24T10:52:06.640+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:06 INFO SparkUI: Stopped Spark web UI at http://2ef039423968:4040
[2025-10-24T10:52:06.853+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:06 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-10-24T10:52:06.948+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:06 INFO MemoryStore: MemoryStore cleared
[2025-10-24T10:52:06.955+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:06 INFO BlockManager: BlockManager stopped
[2025-10-24T10:52:06.971+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:06 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-10-24T10:52:06.984+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:06 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-10-24T10:52:07.024+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:07 INFO SparkContext: Successfully stopped SparkContext
[2025-10-24T10:52:07.740+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:07 INFO ShutdownHookManager: Shutdown hook called
[2025-10-24T10:52:07.744+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-567d850a-5218-4243-8363-218b8301eb61
[2025-10-24T10:52:07.816+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-5afbf267-c8c4-4e41-af4e-cd1801b81250/pyspark-32de1ff9-02c1-4ffa-9d2a-959661e34f4d
[2025-10-24T10:52:07.840+0000] {spark_submit.py:492} INFO - 25/10/24 10:52:07 INFO ShutdownHookManager: Deleting directory /tmp/spark-5afbf267-c8c4-4e41-af4e-cd1801b81250
[2025-10-24T10:52:08.401+0000] {taskinstance.py:1138} INFO - Marking task as SUCCESS. dag_id=load_csv_to_postgres_dag, task_id=spark_load_csv_to_postgres, execution_date=20251024T104927, start_date=20251024T104939, end_date=20251024T105208
[2025-10-24T10:52:09.509+0000] {local_task_job_runner.py:234} INFO - Task exited with return code 0
[2025-10-24T10:52:09.764+0000] {taskinstance.py:3280} INFO - 0 downstream tasks scheduled from follow-on schedule check
